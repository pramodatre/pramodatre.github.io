<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-03-20">

<title>blog - Understanding Object Tracking: A Hands-on Approach, Part 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Pramod Anantharam</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pramodatre" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/pramodatre" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Understanding Object Tracking: A Hands-on Approach, Part 1</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">object tracking</div>
                <div class="quarto-category">tracking by detection</div>
                <div class="quarto-category">perception</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 20, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>There are multiple state-of-the-art approaches for object detection <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. These approaches addresses the perception problem: How do we perceive various objects in the environment? These objects may move or the perceiver may move around relative to the objects in the real-world. Being able to track these objects reliably over time will allow us to predict their next move–this is crucial for autonomous vehicles among many other applications. Most importantly, tracking objects over time will help us create trajectories of various objects. These trajectories can be analyzes to glean behaviors that will help us move from “perception” to “understanding”. Here are some applications that need object tracking: (a) Understanding behaviors of people in enclosed spaces or outdoors for surveillance. (b) Predicting motion of objects for motion planning of an autonomous vehicle (e.g., self-driving cars, drones). (c) Creating object trajectories and provide valuable statistics for different object types (e.g., people and vehicle-type counts, direction of flow of objects).</p>
<p>I was motivated by these applications of object tracking and started reading the chapter on object tracking from Forsyth et. al. <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Here is my attempt to make this chapter more hands-on by applying tracking ideas explained in the chapter to realistic scenes with moving people/objects. I will be presenting some the ideas and corresponding implementation in Python so that you get a good sense of these tracking algorithms. You can apply these ideas to your own projects and start your journey toward “understanding” object behaviors which is quite an exciting topic!</p>
<section id="dataset-preparation" class="level1">
<h1>Dataset Preparation</h1>
<p>Detector based tracking approaches assume that the object detector is good enough to detect objects in almost all frames. In this article, we will focus on detector based tracking since we have such good quality detectors at our disposal. Specifically, we will use YOLOv3 <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> as our detector and PETS2009 as our dataset for evaluating various approaches to tracking. PETS is an acronym for Performance Evaluation of Tracking and Surveillance. This dataset contains image sequence of people walking around in complex patterns crossing each other and ground-truth bounding boxes around people for all the frames. We will limit our exploration of tracking approaches to the ones that are described in <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> though there are advances in tracking approaches as described in <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<section id="clone-the-repo" class="level2">
<h2 class="anchored" data-anchor-id="clone-the-repo">Clone the repo</h2>
<p>First, fetch all the accompanying code from Git using the command:</p>
<pre class="shell"><code>git clone https://github.com/pramodatre/cv-algorithms.git</code></pre>
</section>
<section id="dataset-downloads" class="level2">
<h2 class="anchored" data-anchor-id="dataset-downloads">Dataset downloads</h2>
<p>PETS2009 dataset is collected with specific tasks such as person count and density estimation, people tracking, and flow analysis and event recognition. We will choose S2L1 dataset which is collected for people tracking containing sparse crowd and difficulty level 1 (L1).</p>
<ul>
<li><a href="http://cs.binghamton.edu/~mrldata/public/PETS2009/S2_L1.tar.bz2">PETS2009 image sequence S2L1</a></li>
<li><a href="http://www.milanton.de/files/gt/PETS2009/PETS2009-S2L1.xml">PETS2009 S2L1 ground-truth annotations XML</a></li>
<li><a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg">YOLOv3 cfg file</a></li>
<li><a href="https://pjreddie.com/media/files/yolov3.weights">YOLOv3 weights file</a></li>
</ul>
<p>You can just run <code>python prepare_data.py</code> and the script takes care of downloading and extracting the data for you. If you do not have GPU support on your local machine, it would be too slow to generate detections in real-time using YOLOv3. To circumvent this, we will pre-compute detections and cache them for each image in the ground-truth image-sequence for rapid evaluations and visualization. This will save a lot of time when we wish to visualize results from various tracking approaches.</p>
<section id="visualizing-tracking-ground-truth" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-tracking-ground-truth">Visualizing tracking ground-truth</h3>
<p>You can visualize the tracking ground-truth data using the script <code>play_image_sequence.py</code>. This script takes image sequence directory location and ground-truth XML file as inputs. The image sequence and corresponding bounding boxes are visualized over time - you will be able to see people moving around with bounding boxes around them. Each person is assigned a unique number as they navigate the scene sometime with tangled paths occluding each other.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pets2009S2L1_gt_bboxes.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Visualizing tracking ground-truth where each person is assigned a unique ID. We will evaluate our tracking algorithms on this ground-truth data.</figcaption>
</figure>
</div>
<pre class="shell"><code>python play_image_sequence.py --image_dir './data/Crowd_PETS09/S2/L1/Time_12-34/View_001' --gt './data/PETS2009-S2L1.xml'</code></pre>
</section>
<section id="precomputing-detections" class="level3">
<h3 class="anchored" data-anchor-id="precomputing-detections">Precomputing detections</h3>
<p>Running YOLOv3 without GPU support is quite slow. We will pre-compute all the detections per frame and serialize it indexed by the frame number for repeated access. This helps us in rapid testing of our object tracking algorithms and visualize tracking results. All tracking algorithms are implemented in <code>trackers.py</code> (runs all trackers on ground-truth when you invoke it). When you invoke <code>trackers.py</code> using the following command:</p>
<pre class="shell"><code>python trackers.py --image_dir './data/Crowd_PETS09/S2/L1/Time_12-34/View_001'</code></pre>
<p>there is this section of code which checks for pre-existing detection file.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(saved_detections_file):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Could not find saved detections file: </span><span class="sc">{</span>saved_detections_file<span class="sc">}</span><span class="ss">. Will have to run YOLO on your machine which may be slow the first time. Results will be cached for future runs."</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    yolo <span class="op">=</span> YOLOdetector(image_dir)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    yolo.run()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If such a file doesn’t exist, YOLOv3 model will be used to generate detection per frame and save all the detection and frame index to the file <code>saved_detections_file</code>. We will use <code>trackers.py</code> as our primary script to write various tracking algorithms.</p>
</section>
</section>
</section>
<section id="object-tracking-evaluation-metrics" class="level1">
<h1>Object Tracking Evaluation Metrics</h1>
<p>Before we develop object tracking techniques, we need a way to evaluate and quantify the quality of various trackers. This enables us to compare tracking algorithms and also measure the impact of improvements to the tracking algorithm. The computer vision community has realized the complexity of such an evaluation metric and there is a huge body of research dedicated to developing such evaluation metrics for object tracking <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. This is referred to as MOT (Multiple Object Tracking) metrics. There is an effort to create standardized datasets for benchmarking various tracking algorithms. One such popular benchmark is the Multiple Object Tracking Benchmark <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. One such state-of-the-art more recent metric is Higher Order Tracking Accuracy (HOTA) <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> which is proposed as an improvement over existing Multiple Object Tracking metrics. HOTA metric measures localization, detection, and association accuracies using an Intersection Over Union (IoU) formulation and combines all these three measures into a single score. A single score for a tracker will help us rank tracking approaches with a unified metric. Further, HOTA metric is interpretable, i.e., when you examine the three scores that are combined to create a single HOTA score, you will be able to narrow down the performance issue of the tracker and take steps to improve the tracking algorithms as described here <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. You can read <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> for an intuitive explanation of HOTA and dig deeper into the metric by reading <a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>. In this post, I will focus on using an open-source implementation of HOTA <a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>.</p>
<section id="computing-the-hota-metric" class="level2">
<h2 class="anchored" data-anchor-id="computing-the-hota-metric">Computing the HOTA metric</h2>
<p>We will leverage an open source implementation of HOTA metrics implemented in Python <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> for our evaluation. For the sake of completeness, I will go over the steps even though this is just a repetition of details from <a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. First, let’s clone the repo.</p>
<pre><code>git clone https://github.com/JonathonLuiten/TrackEval.git</code></pre>
<p>There is a single script <code>run_mot_challenge.py</code> that can run various benchmarks like shown on <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>.</p>
<p>Download the data zip file from <a href="https://omnomnom.vision.rwth-aachen.de/data/TrackEval/data.zip">here</a>. Place the uncompressed file in TrackEval directory (root of the cloned repo). Run the <code>run_mot_challenge.py</code> which displays various benchmark evaluation results. This shows that all the files are correctly downloaded and setup.</p>
<pre class="shell"><code>python scripts/run_mot_challenge.py --BENCHMARK MOT17 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL MPNTrack --METRICS HOTA CLEAR Identity VACE --USE_PARALLEL False --NUM_PARALLEL_CORES 1</code></pre>
<p>After you run the above command, you will see various trackers and their evaluation on a benchmark. However, we are interested in running tracking evaluation on our own data, i.e., we would have implemented custom trackers and would like to evaluate their tracking performance on PETS2009 S2L1 ground-truth data. We will have to follow through the instructions outlined here <a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> for computing HOTA metric score.</p>
<p>At first, it may seem a bit confusing to setup the evaluation directories. However, once you setup the evaluation, you can keep adding your custom trackers to evaluate. I will go through the process so that you don’t have to go through the same confusion as I did. Broadly, there are only two steps:</p>
<ul>
<li>Setup benchmark, i.e., ground-truth tracking</li>
<li>Add tracker you want to evaluate</li>
</ul>
<section id="setup-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="setup-benchmark">Setup benchmark</h4>
<p>Ground-truth tracking data is to be placed under <code>TrackEval/data/gt/mot_challenge/&lt;YourChallenge&gt;</code>. In our case, let’s call the new challenge as <code>OBJTR22-train</code>, i.e., create a directory <code>TrackEval/data/gt/mot_challenge/OBJTR22-train/OBJTR22-01/gt</code>. You need to place two files in the directory. <code>gt.txt</code> (look at ground-truth data preparation section) and <code>seqinfo.ini</code> file with the following contents:</p>
<pre><code>[Sequence]
name=OBJTR22
seqLength=795</code></pre>
<p>PETS2009 S2L1 ground-truth data contains <code>795</code> frames hence we have set seqLength to <code>795</code>. Next crete three files <code>OBJTR22-all.txt</code>, <code>OBJTR22-train.txt</code>, and <code>OBJTR22-test.txt</code> with the same following content and place them in <code>TrackEval/data/gt/mot_challenge/seqmaps</code> directory.</p>
<pre><code>name
OBJTR22-01</code></pre>
<p>Now you are all set to use this benchmark (ground-truth) to evaluate trackers we will be implementing in this post.</p>
</section>
<section id="add-tracker-you-want-to-evaluate" class="level4">
<h4 class="anchored" data-anchor-id="add-tracker-you-want-to-evaluate">Add tracker you want to evaluate</h4>
<p>Create a directory <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train</code> where we will place your tracker outputs. To setup our evaluation, we will pretend that we have a perfect tracker, i.e., we will use ground-truth tracking data as our tracker output. Let’s call this tracker <code>PerfectTracker</code>. To add this tracker for evaluation, first, create <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data</code>. Copy the <code>gt.txt</code> to the directory you just created and rename the file to <code>OBJTR22-01.txt</code>.</p>
</section>
<section id="ground-truth-data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="ground-truth-data-preparation">Ground-truth data preparation</h3>
<p>Ground-truth text file <code>gt.txt</code> with ground-truth detections is of the format:</p>
<pre><code>&lt;frame&gt;, &lt;id&gt;, &lt;bb_left&gt;, &lt;bb_top&gt;, &lt;bb_width&gt;, &lt;bb_height&gt;, &lt;conf&gt;, &lt;x&gt;, &lt;y&gt;, &lt;z&gt;</code></pre>
<p>Here the description from the github page of TrackEval: “The world coordinates x,y,z are ignored for the 2D challenge and can be filled with -1. Similarly, the bounding boxes are ignored for the 3D challenge. However, each line is still required to contain 10 values.”</p>
<p>You can export the PETS2009 S2L1 data to the above format by running <code>python convert_to_mot_challenge_format.py</code>. The output file <code>gt.txt</code> will be written to the current directory. If you would like to confirm if <code>gt.txt</code> is correctly exported, you can place the same ground-truth file at <code>TrackEval/data/gt/mot_challenge/OBJTR22-train/OBJTR22-01/gt</code> and <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data</code> and run HOTA metric calculation. Rename the file <code>gt.txt</code> to <code>OBJTR22-01.txt</code> only for the tracker file you place at <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data</code>. So, you will have the same contents of <code>gt.txt</code> in the file <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data/OBJTR22-01.txt</code>.</p>
</section>
<section id="run-the-hota-evaluation-on-a-perfect-tracker" class="level3">
<h3 class="anchored" data-anchor-id="run-the-hota-evaluation-on-a-perfect-tracker">Run the HOTA evaluation on a perfect tracker</h3>
<pre><code>python scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL PerfectTracker --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1

Eval Config:
USE_PARALLEL         : False
NUM_PARALLEL_CORES   : 1
BREAK_ON_ERROR       : True
RETURN_ON_ERROR      : False
LOG_ON_ERROR         : /Users/pramodanantharam/dev/git/TrackEval/error_log.txt
PRINT_RESULTS        : True
PRINT_ONLY_COMBINED  : False
PRINT_CONFIG         : True
TIME_PROGRESS        : True
DISPLAY_LESS_PROGRESS : False
OUTPUT_SUMMARY       : True
OUTPUT_EMPTY_CLASSES : True
OUTPUT_DETAILED      : True
PLOT_CURVES          : True

MotChallenge2DBox Config:
PRINT_CONFIG         : True
GT_FOLDER            : /Users/pramodanantharam/dev/git/TrackEval/data/gt/mot_challenge/
TRACKERS_FOLDER      : /Users/pramodanantharam/dev/git/TrackEval/data/trackers/mot_challenge/
OUTPUT_FOLDER        : None
TRACKERS_TO_EVAL     : ['PerfectTracker']
CLASSES_TO_EVAL      : ['pedestrian']
BENCHMARK            : OBJTR22
SPLIT_TO_EVAL        : train
INPUT_AS_ZIP         : False
DO_PREPROC           : True
TRACKER_SUB_FOLDER   : data
OUTPUT_SUB_FOLDER    :
TRACKER_DISPLAY_NAMES : None
SEQMAP_FOLDER        : None
SEQMAP_FILE          : None
SEQ_INFO             : None
GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt
SKIP_SPLIT_FOL       : False

Evaluating 1 tracker(s) on 1 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count


Evaluating PerfectTracker

    MotChallenge2DBox.get_raw_seq_data(PerfectTracker, OBJTR22-01)         0.1534 sec
    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.2807 sec
    HOTA.eval_sequence()                                                   0.3291 sec
    Count.eval_sequence()                                                  0.0000 sec
1 eval_sequence(OBJTR22-01, PerfectTracker)                              0.7656 sec

All sequences for PerfectTracker finished in 0.77 seconds

HOTA: PerfectTracker-pedestrian    HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)
OBJTR22-01                         100       100       100       100       100       100       100       100       100       100       100       100
COMBINED                           100       100       100       100       100       100       100       100       100       100       100       100

Count: PerfectTracker-pedestrian   Dets      GT_Dets   IDs       GT_IDs
OBJTR22-01                         4650      4650      19        19
COMBINED                           4650      4650      19        19

Timing analysis:
MotChallenge2DBox.get_raw_seq_data                                     0.1534 sec
MotChallenge2DBox.get_preprocessed_seq_data                            0.2807 sec
HOTA.eval_sequence                                                     0.3291 sec
Count.eval_sequence                                                    0.0000 sec
eval_sequence                                                          0.7656 sec
Evaluator.evaluate                                                     1.7024 sec</code></pre>
<p>Notice that the HOTA score is 100 as we expect as we are comparing ground truth tracking data with ground truth itself.</p>
</section>
</section>
</section>
<section id="object-tracking-techniques" class="level1">
<h1>Object Tracking Techniques</h1>
<p>We will go over some the techniques described for object tracking in Chapter 11 of Forsyth et. al. <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>. We will implement these strategies in Python to gain deeper understanding of tracking objects in a real-world setting. We will measure the efficacy of each tracking approach using the PETS2009 tracking ground-truth and HOTA metric.</p>
<section id="baseline-tracker" class="level2">
<h2 class="anchored" data-anchor-id="baseline-tracker">Baseline tracker</h2>
<p>We will use a naive bounding box matching to continue object trajectories as our baseline. In this approach, we will handover bounding boxes from frame <span class="math inline">\((f-1)\)</span> to frame <span class="math inline">\(f\)</span> using max overlap criteria. That is, we will map a bounding box b1 from frame <span class="math inline">\((f-1)\)</span> to a bounding box b2 in frame <span class="math inline">\(f\)</span> if b1 intersection b2 is greater than all other bounding box overlaps (assuming there can be multiple bounding boxes between frames <span class="math inline">\((f-1)\)</span> and <span class="math inline">\(f\)</span>). We intentionally use this as our baseline as it is quite straightforward to implement this idea. Here is the implementation of this idea in a method. This is implemented as <a href="https://github.com/pramodatre/cv-algorithms/blob/master/object_tracking/trackers.py#L101">BaselineTracker</a>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_object_continuation(<span class="va">self</span>, box_t, prev_frame_objects):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Select last position for each object and find overlap to box_t</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    max_i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    overlap_area <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    best_id <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> o_id <span class="kw">in</span> prev_frame_objects:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> prev_frame_objects[o_id]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        xmin, ymin, xmax, ymax <span class="op">=</span> box_t</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        xmin2, ymin2, xmax2, ymax2 <span class="op">=</span> b</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        x_overlap <span class="op">=</span> <span class="bu">min</span>(xmax, xmax2) <span class="op">-</span> <span class="bu">max</span>(xmin, xmin2)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        y_overlap <span class="op">=</span> <span class="bu">min</span>(ymax, ymax2) <span class="op">-</span> <span class="bu">max</span>(ymin, ymin2)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Must check if there is a overlap in x and y direction</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># before computing overlap area. Otherwise, we may end</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># up with +ve area of overlap with both x and y direction</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># overlap is -ve</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x_overlap <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> y_overlap <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>            overlap_area <span class="op">=</span> x_overlap <span class="op">*</span> y_overlap</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> overlap_area <span class="op">&gt;</span> max_i:</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>                max_i <span class="op">=</span> overlap_area</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>                best_id <span class="op">=</span> o_id</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (max_i <span class="op">&gt;</span> <span class="dv">0</span>):</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> best_id</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This method accepts a single bounding box from the current frame and all the bounding boxes from the previous frame as arguments and returns the best object-id in the current frame that is a continuation of the supplied single bounding box. If no such continuation found, this method returns a <code>-1</code>.</p>
<p>When you run the baseline tracking algorithm, a file in a suitable format for HOTA evaluation will be written to <code>tracker_baseline.txt</code>. Place this file at <code>TrackEval/data/trackers/mot_challenge/OBJTR22-train/OBJTR22/data</code>. Rename the file <code>tracker_baseline.txt</code> to <code>OBJTR22-01.txt</code> and run the HOTA evaluation script as shown to compute the HOTA metrics.</p>
<pre><code>python scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL OBJTR22 --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1

...

HOTA: OBJTR22-pedestrian           HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)
OBJTR22-01                         34.46     46.593    25.582    67.789    51.982    28.316    64.04     77.946    41.633    47.954    72.447    34.741
COMBINED                           34.46     46.593    25.582    67.789    51.982    28.316    64.04     77.946    41.633    47.954    72.447    34.741

Count: OBJTR22-pedestrian          Dets      GT_Dets   IDs       GT_IDs
OBJTR22-01                         6064      4650      81        19
COMBINED                           6064      4650      81        19</code></pre>
<p>The baseline tracker we have implemented has a HOTA score of 34.46 which is computed by combining localization accuracy (LocA), detection accuracy (DetA), and association accuracy (AssA). For deeper intuition of HOTA you can read the paper <a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> and also this well written blog post <a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>. To summarize, our baseline tracker has the following scores.</p>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>HOTA</strong></td>
<td><strong>34.46</strong></td>
</tr>
<tr class="even">
<td>LocA</td>
<td>77.946</td>
</tr>
<tr class="odd">
<td>DetA</td>
<td>46.593</td>
</tr>
<tr class="even">
<td>AssA</td>
<td>25.582</td>
</tr>
</tbody>
</table>
<p>We will next implement object tracking algorithms mentioned in the book Forsyth, D., &amp; Ponce, J. (2003) <a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> and evaluate their performance using the HOTA metrics.</p>
</section>
<section id="detection-based-tracking" class="level2">
<h2 class="anchored" data-anchor-id="detection-based-tracking">Detection based tracking</h2>
<p>If we have a good detector, we can use detection based tracking. “Good” detector seems subjective but this approach assumes that each object is detected reliably across frames. Detection based tracking then bridges object movements across frames by matching detections from one frame to the next frame. The matching is done using overlap score and using a bipartite matching to ensure we assign one detection in a frame to a single detection in the next frame.</p>
<p>Here is the tracking by detection Algorithm 11.1 from the book Forsyth, D., &amp; Ponce, J. (2003) <a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>:</p>
<hr>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(x_{k}\)</span>(t) is the <span class="math inline">\(k^{th}\)</span> response of the detector in the <span class="math inline">\(i^{th}\)</span> frame</li>
<li>t(k, i) is the <span class="math inline">\(k^{th}\)</span> track in the <span class="math inline">\(i^{th}\)</span> frame</li>
<li>*t(k, i) is the detector response attached to the <span class="math inline">\(k^{th}\)</span> track in the <span class="math inline">\(i^{th}\)</span> frame</li>
</ul>
<p>Assumption:</p>
<ul>
<li>We have a reasonably reliable detector with distance d such that d(<em>t(k, i), </em>t(k, i-1)) is small. In words, the detector response in <span class="math inline">\(i^{th}\)</span> frame and the previous frame <span class="math inline">\((i-1)^{th}\)</span> frame are quite close.</li>
</ul>
<pre><code>First frame:
&gt; Create a track for each detector response

All other frame:
&gt; Link tracks and detector responses by bipartite matching
&gt; Spawn a new track for each detector response not assigned to a track
&gt; Prune any track that has not received a detector response for some frames

Cleanup:
&gt; We now have trajectories in space time. Link trajectories when
justified (perhaps using a dynamical or appearance model)</code></pre>
<hr>
<p>The above algorithm is implemented as <a href="https://github.com/pramodatre/cv-algorithms/blob/master/object_tracking/trackers.py#L248">DetectionBasedTracker</a>.</p>
<p>If the notation above is confusing, no worries! I would just think in terms of object detections in each frame. We need to assign a detection in a frame to a detection in the next frame – this enables us to track object detection across frames. Here is a method that does exactly this.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_object_continuation_using_bipartite_matching(<span class="va">self</span>, cur_det,obj_map):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Connect objects in previous frame (obj_map) to objects in</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    the current frame (cur_det) using an optimization technique.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        cur_det (list): Containing Detection objects; one</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">            detection object per detection</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        obj_map (dict): Containing object_id as key and</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">            corresponding bounding box as value</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: Updated obj_map</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> obj_map:</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First frame</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> det <span class="kw">in</span> cur_det:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            obj_map[<span class="va">self</span>.o_id_count] <span class="op">=</span> det.get_xmin_ymin_xmax_ymax()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.o_id_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rest of the frames</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        cur_det_dict <span class="op">=</span> {}</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        rows, cols <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(obj_map.keys())), <span class="bu">len</span>(cur_det)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        cost_matrix <span class="op">=</span> np.zeros((rows, cols))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        cost_martix_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>cost_matrix, index<span class="op">=</span><span class="bu">list</span>(obj_map.keys()), columns<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(cols))</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read all detections to a dictionary</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        det_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c_det <span class="kw">in</span> cur_det:</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            det_bbox <span class="op">=</span> c_det.get_xmin_ymin_xmax_ymax()</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(det_bbox)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            cur_det_dict[det_count] <span class="op">=</span> det_bbox</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            det_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> o_id <span class="kw">in</span> obj_map:</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> det_id <span class="kw">in</span> cur_det_dict:</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                det_bbox <span class="op">=</span> cur_det_dict[det_id]</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                iou_score <span class="op">=</span> <span class="va">self</span>.compute_iou_score(obj_map[o_id], det_bbox)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> iou_score <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>                    cost_martix_df.loc[o_id, det_id] <span class="op">=</span> iou_score</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        cost_martix_array <span class="op">=</span> cost_martix_df.values <span class="op">*</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        row_ind, col_ind <span class="op">=</span> linear_sum_assignment(cost_martix_array)</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update object map with best assignments</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, j <span class="kw">in</span> <span class="bu">zip</span>(row_ind, col_ind):</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            obj_id <span class="op">=</span> <span class="bu">list</span>(cost_martix_df.index)[i]</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            det_id <span class="op">=</span> cost_martix_df.columns[j]</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cost_martix_array[i, j] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>                obj_map[<span class="va">self</span>.o_id_count] <span class="op">=</span> cur_det_dict[det_id]</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.o_id_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>                obj_map[obj_id] <span class="op">=</span> cur_det_dict[det_id]</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> obj_map</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This method predicts object continuations from one frame to the next using an optimization approach called Hungarian Optimization a.k.a. linear sum assignment or bipartite matching. I wanted to point out some important implementation details here. For complete code, please refer to <a href="https://github.com/pramodatre/cv-algorithms/blob/master/object_tracking/trackers.py#L248">github repo</a>. We maintain a dictionary for any object that is currently in the frame. The dictionary has object ID as key and its bounding box as value. This dictionary is updated within this method and the updated dictionary is returned.</p>
<p>We are using a global counter (self.o_id_count) which is a class attribute to track object ID and increment it for new object ID assignments. A new object ID is assigned to a detection in the current frame when the cost matrix has zero entry for all object bounding boxes in the previous frame. This may need a bit of explanation! The cost matrix is updated using the Intersection over Union (IoU) scores between an object detection bounding box in the previous frame and all object detection bounding boxes in the current frame. Since the optimization minimizes overall cost of assigning objects from the previous frame to the objects in current frame, we will have to use a negative of the IoU scores. That is, we prefer object assignments with highest IoU scores and this translates to the least cost when we take negative of IoU scores.</p>
<p>The cleanup step in the above algorithm description is implemented in <code>prune_tracks</code> method as shown below. Notice that we had to pick number of frames for which an object position was not updated to prune them – set as a global variable <code>STALE_DET_THRESHOLD_FRAMES</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prune_tracks(<span class="va">self</span>, cur_obj_map, prev_obj_map):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Remove objects whose positions are not updated for certain frames.</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">        cur_obj_map (dict): Object id and corresponding</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">                    detection bounding box</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">        prev_obj_map (dict): Previous frame object id and</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">                     corresponding bounding box</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: Pruned cur_obj_map</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"comparing </span><span class="sc">{</span>cur_obj_map<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>prev_obj_map<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cur_id <span class="kw">in</span> cur_obj_map:</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cur_id <span class="kw">in</span> prev_obj_map:</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cur_obj_map[cur_id] <span class="op">==</span> prev_obj_map[cur_id]:</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.o_ids_without_updates_counts[cur_id] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    keys_to_remove <span class="op">=</span> []</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cur_id <span class="kw">in</span> <span class="va">self</span>.o_ids_without_updates_counts:</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.o_ids_without_updates_counts[cur_id]</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>            <span class="op">&gt;</span> <span class="va">self</span>.STALE_DET_THRESHOLD_FRAMES</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        ):</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>            keys_to_remove.append(cur_id)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cur_id <span class="kw">in</span> keys_to_remove:</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> cur_obj_map[cur_id]</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="va">self</span>.o_ids_without_updates_counts[cur_id]</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cur_obj_map</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here is the HOTA evaluation for the implementation of detection based tracking approach.</p>
<pre><code>python scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL TrackByDetection --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1

Eval Config:
USE_PARALLEL         : False
NUM_PARALLEL_CORES   : 1
BREAK_ON_ERROR       : True
RETURN_ON_ERROR      : False
LOG_ON_ERROR         : /Users/pramodanantharam/dev/git/TrackEval/error_log.txt
PRINT_RESULTS        : True
PRINT_ONLY_COMBINED  : False
PRINT_CONFIG         : True
TIME_PROGRESS        : True
DISPLAY_LESS_PROGRESS : False
OUTPUT_SUMMARY       : True
OUTPUT_EMPTY_CLASSES : True
OUTPUT_DETAILED      : True
PLOT_CURVES          : True

MotChallenge2DBox Config:
PRINT_CONFIG         : True
GT_FOLDER            : /Users/pramodanantharam/dev/git/TrackEval/data/gt/mot_challenge/
TRACKERS_FOLDER      : /Users/pramodanantharam/dev/git/TrackEval/data/trackers/mot_challenge/
OUTPUT_FOLDER        : None
TRACKERS_TO_EVAL     : ['TrackByDetection']
CLASSES_TO_EVAL      : ['pedestrian']
BENCHMARK            : OBJTR22
SPLIT_TO_EVAL        : train
INPUT_AS_ZIP         : False
DO_PREPROC           : True
TRACKER_SUB_FOLDER   : data
OUTPUT_SUB_FOLDER    :
TRACKER_DISPLAY_NAMES : None
SEQMAP_FOLDER        : None
SEQMAP_FILE          : None
SEQ_INFO             : None
GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt
SKIP_SPLIT_FOL       : False

Evaluating 1 tracker(s) on 1 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count


Evaluating TrackByDetection

    MotChallenge2DBox.get_raw_seq_data(TrackByDetection, OBJTR22-01)       0.1711 sec
    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.2702 sec
    HOTA.eval_sequence()                                                   0.3073 sec
    Count.eval_sequence()                                                  0.0000 sec
1 eval_sequence(OBJTR22-01, TrackByDetection)                            0.7511 sec

All sequences for TrackByDetection finished in 0.75 seconds

HOTA: TrackByDetection-pedestrian  HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)
OBJTR22-01                         39.458    48.459    32.238    67.952    53.977    39.871    49.078    77.514    46.798    55.504    71.343    39.598
COMBINED                           39.458    48.459    32.238    67.952    53.977    39.871    49.078    77.514    46.798    55.504    71.343    39.598

Count: TrackByDetection-pedestrian Dets      GT_Dets   IDs       GT_IDs
OBJTR22-01                         5854      4650      83        19
COMBINED                           5854      4650      83        19

Timing analysis:
MotChallenge2DBox.get_raw_seq_data                                     0.1711 sec
MotChallenge2DBox.get_preprocessed_seq_data                            0.2702 sec
HOTA.eval_sequence                                                     0.3073 sec
Count.eval_sequence                                                    0.0000 sec
eval_sequence                                                          0.7511 sec
Evaluator.evaluate                                                     1.3774 sec</code></pre>
<p>Observe that there are improvements to tracking throughout! Here is the evaluation summary with percentage improvements shown from the baseline tracker. Biggest gain is in association accuracy which makes sense since the detector based approach is using an optimization approach to assign object continuations between frames minimizing the overall cost of assignments. The baseline approach is doing this assignment without using a greedy approach by selecting the highest overlapping bounding box across frames.</p>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>HOTA</strong></td>
<td><strong>39.45</strong> (<span class="math inline">\(\uparrow\)</span> 14%)</td>
</tr>
<tr class="even">
<td>LocA</td>
<td>77.514 (<span class="math inline">\(\downarrow\)</span> 0.5%)</td>
</tr>
<tr class="odd">
<td>DetA</td>
<td>48.459 (<span class="math inline">\(\uparrow\)</span> 4%)</td>
</tr>
<tr class="even">
<td>AssA</td>
<td>32.238 (<span class="math inline">\(\uparrow\)</span> 26%)</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>HOTA metrics for tracking by detection approach.</p>
</blockquote>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, you were able to understanding basic tracking strategies, setup ground-truth data, select evaluation metrics, and implement BaselineTracker and DetectionBasedTracker in Python. You are able to quantify the improvements you made from baseline tracker to detection based tracker using HOTA metrics. Having such a metric that measure our progress as we refine our approach is crucial for staying focused in our attempt to improve our approach to object tracking. This metric provide direct feedback to us allowing us to measure success of new ideas and thereby iterate quickly to improve our approach. In future posts, I will go though the implementation of more tracking approaches such as matching by pixel similarity and motion models presented in Forsyth et. al. <a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>.</p>
<p><strong>Code used in this blog post is <a href="https://github.com/pramodatre/cv-algorithms/tree/master/object_tracking">here</a></strong></p>
</section>
<section id="references" class="level1">
<h1>References</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://paperswithcode.com/task/object-detection">Object Detection Papers with Code</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://blog.netcetera.com/object-detection-and-tracking-in-2020-f10fb6ff9af3">Object Detection and Tracking in 2020</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Forsyth, D., &amp; Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Redmon, J., &amp; Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://pjreddie.com/darknet/yolo/">YOLO: Real-Time Object Detection</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Forsyth, D., &amp; Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://blog.netcetera.com/object-detection-and-tracking-in-2020-f10fb6ff9af3">Object Detection and Tracking in 2020</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Yin, Fei &amp; Makris, Dimitrios &amp; Velastin, Sergio. (2007). Performance evaluation of object tracking algorithms. 10th IEEE International Workshop on Performance Evaluation of Tracking and Surveillance (PETS2007).<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Luiten, J., Os̆ep, A., Dendorfer, P. et al.&nbsp;HOTA: A Higher Order Metric for Evaluating Multi-object Tracking. Int J Comput Vis 129, 548–578 (2021). https://doi.org/10.1007/s11263-020-01375-2<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://motchallenge.net/">Multiple object Tracking Benchmark</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><a href="https://github.com/JonathonLuiten/TrackEval">Jonathon Luiten, Arne Hoffhues, TrackEval</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Jonathon Luiten, A.O. &amp; Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1">How to evaluate tracking with the HOTA metrics</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1">How to evaluate tracking with the HOTA metrics</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p><a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1">How to evaluate tracking with the HOTA metrics</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Jonathon Luiten, A.O. &amp; Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><a href="https://github.com/JonathonLuiten/TrackEval/tree/master/docs/MOTChallenge-Official">MOT Challenge Metrics Implementation</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><a href="https://github.com/JonathonLuiten/TrackEval/blob/master/trackeval/metrics/hota.py">HOTA Metric Implementation in Python</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><a href="https://github.com/JonathonLuiten/TrackEval/tree/master/docs/MOTChallenge-Official#evaluating-on-your-own-data">Tracking Evaluation on Your Own Data</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p><a href="https://github.com/JonathonLuiten/TrackEval/tree/master/docs/MOTChallenge-Official">MOT Challenge Metrics Implementation</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><a href="https://github.com/JonathonLuiten/TrackEval/tree/master/docs/MOTChallenge-Official#evaluating-on-your-own-data">Tracking Evaluation on Your Own Data</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>Forsyth, D., &amp; Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1">How to evaluate tracking with the HOTA metrics</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>Jonathon Luiten, A.O. &amp; Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>Redmon, J., &amp; Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>Redmon, J., &amp; Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>Forsyth, D., &amp; Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>