[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! I‚Äôm Pramod üëã weclome to my blog!",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 20, 2022\n\n\nUnderstanding Object Tracking: A Hands-on Approach, Part 1\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nLive Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone\n\n\n\n\n\n\n\nDec 21, 2020\n\n\nSimultaneous Localization and Mapping (SLAM) with Crazyflie\n\n\n\n\n\n\n\nDec 20, 2020\n\n\nPorting Motion Planning Project to Crazyflie\n\n\n\n\n\n\n\nNov 6, 2020\n\n\nPorting Backyard Flyer Project to Crazyflie\n\n\n\n\n\n\n\nOct 2, 2020\n\n\nCrazyflie2.1 Micro-UAV Assembly Experience\n\n\n\n\n\n\n\nMay 17, 2020\n\n\nDisparity Map Computation in Python and C++\n\n\n\n\n\n\n\nMay 10, 2020\n\n\nBuilding Tensorflow from Source on MacOS\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "",
    "text": "There are multiple state-of-the-art approaches for object detection 1 2. These approaches addresses the perception problem: How do we perceive various objects in the environment? These objects may move or the perceiver may move around relative to the objects in the real-world. Being able to track these objects reliably over time will allow us to predict their next move‚Äìthis is crucial for autonomous vehicles among many other applications. Most importantly, tracking objects over time will help us create trajectories of various objects. These trajectories can be analyzes to glean behaviors that will help us move from ‚Äúperception‚Äù to ‚Äúunderstanding‚Äù. Here are some applications that need object tracking: (a) Understanding behaviors of people in enclosed spaces or outdoors for surveillance. (b) Predicting motion of objects for motion planning of an autonomous vehicle (e.g., self-driving cars, drones). (c) Creating object trajectories and provide valuable statistics for different object types (e.g., people and vehicle-type counts, direction of flow of objects).\nI was motivated by these applications of object tracking and started reading the chapter on object tracking from Forsyth et. al. 3. Here is my attempt to make this chapter more hands-on by applying tracking ideas explained in the chapter to realistic scenes with moving people/objects. I will be presenting some the ideas and corresponding implementation in Python so that you get a good sense of these tracking algorithms. You can apply these ideas to your own projects and start your journey toward ‚Äúunderstanding‚Äù object behaviors which is quite an exciting topic!"
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#clone-the-repo",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#clone-the-repo",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Clone the repo",
    "text": "Clone the repo\nFirst, fetch all the accompanying code from Git using the command:\ngit clone https://github.com/pramodatre/cv-algorithms.git"
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#dataset-downloads",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#dataset-downloads",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Dataset downloads",
    "text": "Dataset downloads\nPETS2009 dataset is collected with specific tasks such as person count and density estimation, people tracking, and flow analysis and event recognition. We will choose S2L1 dataset which is collected for people tracking containing sparse crowd and difficulty level 1 (L1).\n\nPETS2009 image sequence S2L1\nPETS2009 S2L1 ground-truth annotations XML\nYOLOv3 cfg file\nYOLOv3 weights file\n\nYou can just run python prepare_data.py and the script takes care of downloading and extracting the data for you. If you do not have GPU support on your local machine, it would be too slow to generate detections in real-time using YOLOv3. To circumvent this, we will pre-compute detections and cache them for each image in the ground-truth image-sequence for rapid evaluations and visualization. This will save a lot of time when we wish to visualize results from various tracking approaches.\n\nVisualizing tracking ground-truth\nYou can visualize the tracking ground-truth data using the script play_image_sequence.py. This script takes image sequence directory location and ground-truth XML file as inputs. The image sequence and corresponding bounding boxes are visualized over time - you will be able to see people moving around with bounding boxes around them. Each person is assigned a unique number as they navigate the scene sometime with tangled paths occluding each other.\n\n\n\nVisualizing tracking ground-truth where each person is assigned a unique ID. We will evaluate our tracking algorithms on this ground-truth data.\n\n\npython play_image_sequence.py --image_dir './data/Crowd_PETS09/S2/L1/Time_12-34/View_001' --gt './data/PETS2009-S2L1.xml'\n\n\nPrecomputing detections\nRunning YOLOv3 without GPU support is quite slow. We will pre-compute all the detections per frame and serialize it indexed by the frame number for repeated access. This helps us in rapid testing of our object tracking algorithms and visualize tracking results. All tracking algorithms are implemented in trackers.py (runs all trackers on ground-truth when you invoke it). When you invoke trackers.py using the following command:\npython trackers.py --image_dir './data/Crowd_PETS09/S2/L1/Time_12-34/View_001'\nthere is this section of code which checks for pre-existing detection file.\nif not os.path.exists(saved_detections_file):\n    print(\n        f\"Could not find saved detections file: {saved_detections_file}. Will have to run YOLO on your machine which may be slow the first time. Results will be cached for future runs.\"\n    )\n    yolo = YOLOdetector(image_dir)\n    yolo.run()\nIf such a file doesn‚Äôt exist, YOLOv3 model will be used to generate detection per frame and save all the detection and frame index to the file saved_detections_file. We will use trackers.py as our primary script to write various tracking algorithms."
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#computing-the-hota-metric",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#computing-the-hota-metric",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Computing the HOTA metric",
    "text": "Computing the HOTA metric\nWe will leverage an open source implementation of HOTA metrics implemented in Python 18 for our evaluation. For the sake of completeness, I will go over the steps even though this is just a repetition of details from 19. First, let‚Äôs clone the repo.\ngit clone https://github.com/JonathonLuiten/TrackEval.git\nThere is a single script run_mot_challenge.py that can run various benchmarks like shown on 20.\nDownload the data zip file from here. Place the uncompressed file in TrackEval directory (root of the cloned repo). Run the run_mot_challenge.py which displays various benchmark evaluation results. This shows that all the files are correctly downloaded and setup.\npython scripts/run_mot_challenge.py --BENCHMARK MOT17 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL MPNTrack --METRICS HOTA CLEAR Identity VACE --USE_PARALLEL False --NUM_PARALLEL_CORES 1\nAfter you run the above command, you will see various trackers and their evaluation on a benchmark. However, we are interested in running tracking evaluation on our own data, i.e., we would have implemented custom trackers and would like to evaluate their tracking performance on PETS2009 S2L1 ground-truth data. We will have to follow through the instructions outlined here 21 for computing HOTA metric score.\nAt first, it may seem a bit confusing to setup the evaluation directories. However, once you setup the evaluation, you can keep adding your custom trackers to evaluate. I will go through the process so that you don‚Äôt have to go through the same confusion as I did. Broadly, there are only two steps:\n\nSetup benchmark, i.e., ground-truth tracking\nAdd tracker you want to evaluate\n\n\nSetup benchmark\nGround-truth tracking data is to be placed under TrackEval/data/gt/mot_challenge/&lt;YourChallenge&gt;. In our case, let‚Äôs call the new challenge as OBJTR22-train, i.e., create a directory TrackEval/data/gt/mot_challenge/OBJTR22-train/OBJTR22-01/gt. You need to place two files in the directory. gt.txt (look at ground-truth data preparation section) and seqinfo.ini file with the following contents:\n[Sequence]\nname=OBJTR22\nseqLength=795\nPETS2009 S2L1 ground-truth data contains 795 frames hence we have set seqLength to 795. Next crete three files OBJTR22-all.txt, OBJTR22-train.txt, and OBJTR22-test.txt with the same following content and place them in TrackEval/data/gt/mot_challenge/seqmaps directory.\nname\nOBJTR22-01\nNow you are all set to use this benchmark (ground-truth) to evaluate trackers we will be implementing in this post.\n\n\nAdd tracker you want to evaluate\nCreate a directory TrackEval/data/trackers/mot_challenge/OBJTR22-train where we will place your tracker outputs. To setup our evaluation, we will pretend that we have a perfect tracker, i.e., we will use ground-truth tracking data as our tracker output. Let‚Äôs call this tracker PerfectTracker. To add this tracker for evaluation, first, create TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data. Copy the gt.txt to the directory you just created and rename the file to OBJTR22-01.txt.\n\n\nGround-truth data preparation\nGround-truth text file gt.txt with ground-truth detections is of the format:\n&lt;frame&gt;, &lt;id&gt;, &lt;bb_left&gt;, &lt;bb_top&gt;, &lt;bb_width&gt;, &lt;bb_height&gt;, &lt;conf&gt;, &lt;x&gt;, &lt;y&gt;, &lt;z&gt;\nHere the description from the github page of TrackEval: ‚ÄúThe world coordinates x,y,z are ignored for the 2D challenge and can be filled with -1. Similarly, the bounding boxes are ignored for the 3D challenge. However, each line is still required to contain 10 values.‚Äù\nYou can export the PETS2009 S2L1 data to the above format by running python convert_to_mot_challenge_format.py. The output file gt.txt will be written to the current directory. If you would like to confirm if gt.txt is correctly exported, you can place the same ground-truth file at TrackEval/data/gt/mot_challenge/OBJTR22-train/OBJTR22-01/gt and TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data and run HOTA metric calculation. Rename the file gt.txt to OBJTR22-01.txt only for the tracker file you place at TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data. So, you will have the same contents of gt.txt in the file TrackEval/data/trackers/mot_challenge/OBJTR22-train/PerfectTracker/data/OBJTR22-01.txt.\n\n\nRun the HOTA evaluation on a perfect tracker\npython scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL PerfectTracker --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1\n\nEval Config:\nUSE_PARALLEL         : False\nNUM_PARALLEL_CORES   : 1\nBREAK_ON_ERROR       : True\nRETURN_ON_ERROR      : False\nLOG_ON_ERROR         : /Users/pramodanantharam/dev/git/TrackEval/error_log.txt\nPRINT_RESULTS        : True\nPRINT_ONLY_COMBINED  : False\nPRINT_CONFIG         : True\nTIME_PROGRESS        : True\nDISPLAY_LESS_PROGRESS : False\nOUTPUT_SUMMARY       : True\nOUTPUT_EMPTY_CLASSES : True\nOUTPUT_DETAILED      : True\nPLOT_CURVES          : True\n\nMotChallenge2DBox Config:\nPRINT_CONFIG         : True\nGT_FOLDER            : /Users/pramodanantharam/dev/git/TrackEval/data/gt/mot_challenge/\nTRACKERS_FOLDER      : /Users/pramodanantharam/dev/git/TrackEval/data/trackers/mot_challenge/\nOUTPUT_FOLDER        : None\nTRACKERS_TO_EVAL     : ['PerfectTracker']\nCLASSES_TO_EVAL      : ['pedestrian']\nBENCHMARK            : OBJTR22\nSPLIT_TO_EVAL        : train\nINPUT_AS_ZIP         : False\nDO_PREPROC           : True\nTRACKER_SUB_FOLDER   : data\nOUTPUT_SUB_FOLDER    :\nTRACKER_DISPLAY_NAMES : None\nSEQMAP_FOLDER        : None\nSEQMAP_FILE          : None\nSEQ_INFO             : None\nGT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt\nSKIP_SPLIT_FOL       : False\n\nEvaluating 1 tracker(s) on 1 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count\n\n\nEvaluating PerfectTracker\n\n    MotChallenge2DBox.get_raw_seq_data(PerfectTracker, OBJTR22-01)         0.1534 sec\n    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.2807 sec\n    HOTA.eval_sequence()                                                   0.3291 sec\n    Count.eval_sequence()                                                  0.0000 sec\n1 eval_sequence(OBJTR22-01, PerfectTracker)                              0.7656 sec\n\nAll sequences for PerfectTracker finished in 0.77 seconds\n\nHOTA: PerfectTracker-pedestrian    HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)\nOBJTR22-01                         100       100       100       100       100       100       100       100       100       100       100       100\nCOMBINED                           100       100       100       100       100       100       100       100       100       100       100       100\n\nCount: PerfectTracker-pedestrian   Dets      GT_Dets   IDs       GT_IDs\nOBJTR22-01                         4650      4650      19        19\nCOMBINED                           4650      4650      19        19\n\nTiming analysis:\nMotChallenge2DBox.get_raw_seq_data                                     0.1534 sec\nMotChallenge2DBox.get_preprocessed_seq_data                            0.2807 sec\nHOTA.eval_sequence                                                     0.3291 sec\nCount.eval_sequence                                                    0.0000 sec\neval_sequence                                                          0.7656 sec\nEvaluator.evaluate                                                     1.7024 sec\nNotice that the HOTA score is 100 as we expect as we are comparing ground truth tracking data with ground truth itself."
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#baseline-tracker",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#baseline-tracker",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Baseline tracker",
    "text": "Baseline tracker\nWe will use a naive bounding box matching to continue object trajectories as our baseline. In this approach, we will handover bounding boxes from frame \\((f-1)\\) to frame \\(f\\) using max overlap criteria. That is, we will map a bounding box b1 from frame \\((f-1)\\) to a bounding box b2 in frame \\(f\\) if b1 intersection b2 is greater than all other bounding box overlaps (assuming there can be multiple bounding boxes between frames \\((f-1)\\) and \\(f\\)). We intentionally use this as our baseline as it is quite straightforward to implement this idea. Here is the implementation of this idea in a method. This is implemented as BaselineTracker.\ndef predict_object_continuation(self, box_t, prev_frame_objects):\n    # Select last position for each object and find overlap to box_t\n    max_i = 0\n    overlap_area = 0\n    best_id = -1\n    for o_id in prev_frame_objects:\n        b = prev_frame_objects[o_id]\n        xmin, ymin, xmax, ymax = box_t\n        xmin2, ymin2, xmax2, ymax2 = b\n        x_overlap = min(xmax, xmax2) - max(xmin, xmin2)\n        y_overlap = min(ymax, ymax2) - max(ymin, ymin2)\n        # Must check if there is a overlap in x and y direction\n        # before computing overlap area. Otherwise, we may end\n        # up with +ve area of overlap with both x and y direction\n        # overlap is -ve\n        if x_overlap &gt; 0 and y_overlap &gt; 0:\n            overlap_area = x_overlap * y_overlap\n            if overlap_area &gt; max_i:\n                max_i = overlap_area\n                best_id = o_id\n            else:\n                continue\n    if (max_i &gt; 0):\n        return best_id\n    return -1\nThis method accepts a single bounding box from the current frame and all the bounding boxes from the previous frame as arguments and returns the best object-id in the current frame that is a continuation of the supplied single bounding box. If no such continuation found, this method returns a -1.\nWhen you run the baseline tracking algorithm, a file in a suitable format for HOTA evaluation will be written to tracker_baseline.txt. Place this file at TrackEval/data/trackers/mot_challenge/OBJTR22-train/OBJTR22/data. Rename the file tracker_baseline.txt to OBJTR22-01.txt and run the HOTA evaluation script as shown to compute the HOTA metrics.\npython scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL OBJTR22 --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1\n\n...\n\nHOTA: OBJTR22-pedestrian           HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)\nOBJTR22-01                         34.46     46.593    25.582    67.789    51.982    28.316    64.04     77.946    41.633    47.954    72.447    34.741\nCOMBINED                           34.46     46.593    25.582    67.789    51.982    28.316    64.04     77.946    41.633    47.954    72.447    34.741\n\nCount: OBJTR22-pedestrian          Dets      GT_Dets   IDs       GT_IDs\nOBJTR22-01                         6064      4650      81        19\nCOMBINED                           6064      4650      81        19\nThe baseline tracker we have implemented has a HOTA score of 34.46 which is computed by combining localization accuracy (LocA), detection accuracy (DetA), and association accuracy (AssA). For deeper intuition of HOTA you can read the paper 23 and also this well written blog post 24. To summarize, our baseline tracker has the following scores.\n\n\n\nMetric\nScore\n\n\n\n\nHOTA\n34.46\n\n\nLocA\n77.946\n\n\nDetA\n46.593\n\n\nAssA\n25.582\n\n\n\nWe will next implement object tracking algorithms mentioned in the book Forsyth, D., & Ponce, J. (2003) 25 and evaluate their performance using the HOTA metrics."
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#detection-based-tracking",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#detection-based-tracking",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Detection based tracking",
    "text": "Detection based tracking\nIf we have a good detector, we can use detection based tracking. ‚ÄúGood‚Äù detector seems subjective but this approach assumes that each object is detected reliably across frames. Detection based tracking then bridges object movements across frames by matching detections from one frame to the next frame. The matching is done using overlap score and using a bipartite matching to ensure we assign one detection in a frame to a single detection in the next frame.\nHere is the tracking by detection Algorithm 11.1 from the book Forsyth, D., & Ponce, J. (2003) 26:\n\nNotation:\n\n\\(x_{k}\\)(t) is the \\(k^{th}\\) response of the detector in the \\(i^{th}\\) frame\nt(k, i) is the \\(k^{th}\\) track in the \\(i^{th}\\) frame\n*t(k, i) is the detector response attached to the \\(k^{th}\\) track in the \\(i^{th}\\) frame\n\nAssumption:\n\nWe have a reasonably reliable detector with distance d such that d(t(k, i), t(k, i-1)) is small. In words, the detector response in \\(i^{th}\\) frame and the previous frame \\((i-1)^{th}\\) frame are quite close.\n\nFirst frame:\n&gt; Create a track for each detector response\n\nAll other frame:\n&gt; Link tracks and detector responses by bipartite matching\n&gt; Spawn a new track for each detector response not assigned to a track\n&gt; Prune any track that has not received a detector response for some frames\n\nCleanup:\n&gt; We now have trajectories in space time. Link trajectories when\njustified (perhaps using a dynamical or appearance model)\n\nThe above algorithm is implemented as DetectionBasedTracker.\nIf the notation above is confusing, no worries! I would just think in terms of object detections in each frame. We need to assign a detection in a frame to a detection in the next frame ‚Äì this enables us to track object detection across frames. Here is a method that does exactly this.\ndef predict_object_continuation_using_bipartite_matching(self, cur_det,obj_map):\n    \"\"\"Connect objects in previous frame (obj_map) to objects in\n    the current frame (cur_det) using an optimization technique.\n    Args:\n        cur_det (list): Containing Detection objects; one\n            detection object per detection\n        obj_map (dict): Containing object_id as key and\n            corresponding bounding box as value\n    Returns:\n        dict: Updated obj_map\n    \"\"\"\n    if not obj_map:\n        # First frame\n        for det in cur_det:\n            obj_map[self.o_id_count] = det.get_xmin_ymin_xmax_ymax()\n            self.o_id_count += 1\n    else:\n        # Rest of the frames\n        cur_det_dict = {}\n        rows, cols = len(list(obj_map.keys())), len(cur_det)\n        cost_matrix = np.zeros((rows, cols))\n        cost_martix_df = pd.DataFrame(\n            data=cost_matrix, index=list(obj_map.keys()), columns=list(range(cols))\n        )\n        # read all detections to a dictionary\n        det_count = 0\n        for c_det in cur_det:\n            det_bbox = c_det.get_xmin_ymin_xmax_ymax()\n            print(det_bbox)\n            cur_det_dict[det_count] = det_bbox\n            det_count += 1\n        for o_id in obj_map:\n            for det_id in cur_det_dict:\n                det_bbox = cur_det_dict[det_id]\n                iou_score = self.compute_iou_score(obj_map[o_id], det_bbox)\n                if iou_score &gt; 0:\n                    cost_martix_df.loc[o_id, det_id] = iou_score\n        cost_martix_array = cost_martix_df.values * -1\n        row_ind, col_ind = linear_sum_assignment(cost_martix_array)\n        # Update object map with best assignments\n        for i, j in zip(row_ind, col_ind):\n            obj_id = list(cost_martix_df.index)[i]\n            det_id = cost_martix_df.columns[j]\n            if cost_martix_array[i, j] == 0:\n                obj_map[self.o_id_count] = cur_det_dict[det_id]\n                self.o_id_count += 1\n            else:\n                obj_map[obj_id] = cur_det_dict[det_id]\n    return obj_map\nThis method predicts object continuations from one frame to the next using an optimization approach called Hungarian Optimization a.k.a. linear sum assignment or bipartite matching. I wanted to point out some important implementation details here. For complete code, please refer to github repo. We maintain a dictionary for any object that is currently in the frame. The dictionary has object ID as key and its bounding box as value. This dictionary is updated within this method and the updated dictionary is returned.\nWe are using a global counter (self.o_id_count) which is a class attribute to track object ID and increment it for new object ID assignments. A new object ID is assigned to a detection in the current frame when the cost matrix has zero entry for all object bounding boxes in the previous frame. This may need a bit of explanation! The cost matrix is updated using the Intersection over Union (IoU) scores between an object detection bounding box in the previous frame and all object detection bounding boxes in the current frame. Since the optimization minimizes overall cost of assigning objects from the previous frame to the objects in current frame, we will have to use a negative of the IoU scores. That is, we prefer object assignments with highest IoU scores and this translates to the least cost when we take negative of IoU scores.\nThe cleanup step in the above algorithm description is implemented in prune_tracks method as shown below. Notice that we had to pick number of frames for which an object position was not updated to prune them ‚Äì set as a global variable STALE_DET_THRESHOLD_FRAMES.\ndef prune_tracks(self, cur_obj_map, prev_obj_map):\n    \"\"\"Remove objects whose positions are not updated for certain frames.\n\n    Args:\n        cur_obj_map (dict): Object id and corresponding\n                    detection bounding box\n        prev_obj_map (dict): Previous frame object id and\n                     corresponding bounding box\n\n    Returns:\n        dict: Pruned cur_obj_map\n    \"\"\"\n    print(f\"comparing {cur_obj_map} and {prev_obj_map}\")\n    for cur_id in cur_obj_map:\n        if cur_id in prev_obj_map:\n            if cur_obj_map[cur_id] == prev_obj_map[cur_id]:\n                self.o_ids_without_updates_counts[cur_id] += 1\n\n    keys_to_remove = []\n    for cur_id in self.o_ids_without_updates_counts:\n        if (\n            self.o_ids_without_updates_counts[cur_id]\n            &gt; self.STALE_DET_THRESHOLD_FRAMES\n        ):\n            keys_to_remove.append(cur_id)\n\n    for cur_id in keys_to_remove:\n        del cur_obj_map[cur_id]\n        del self.o_ids_without_updates_counts[cur_id]\n\n    return cur_obj_map\nHere is the HOTA evaluation for the implementation of detection based tracking approach.\npython scripts/run_mot_challenge.py --BENCHMARK OBJTR22 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL TrackByDetection --METRICS HOTA --USE_PARALLEL False --NUM_PARALLEL_CORES 1\n\nEval Config:\nUSE_PARALLEL         : False\nNUM_PARALLEL_CORES   : 1\nBREAK_ON_ERROR       : True\nRETURN_ON_ERROR      : False\nLOG_ON_ERROR         : /Users/pramodanantharam/dev/git/TrackEval/error_log.txt\nPRINT_RESULTS        : True\nPRINT_ONLY_COMBINED  : False\nPRINT_CONFIG         : True\nTIME_PROGRESS        : True\nDISPLAY_LESS_PROGRESS : False\nOUTPUT_SUMMARY       : True\nOUTPUT_EMPTY_CLASSES : True\nOUTPUT_DETAILED      : True\nPLOT_CURVES          : True\n\nMotChallenge2DBox Config:\nPRINT_CONFIG         : True\nGT_FOLDER            : /Users/pramodanantharam/dev/git/TrackEval/data/gt/mot_challenge/\nTRACKERS_FOLDER      : /Users/pramodanantharam/dev/git/TrackEval/data/trackers/mot_challenge/\nOUTPUT_FOLDER        : None\nTRACKERS_TO_EVAL     : ['TrackByDetection']\nCLASSES_TO_EVAL      : ['pedestrian']\nBENCHMARK            : OBJTR22\nSPLIT_TO_EVAL        : train\nINPUT_AS_ZIP         : False\nDO_PREPROC           : True\nTRACKER_SUB_FOLDER   : data\nOUTPUT_SUB_FOLDER    :\nTRACKER_DISPLAY_NAMES : None\nSEQMAP_FOLDER        : None\nSEQMAP_FILE          : None\nSEQ_INFO             : None\nGT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt\nSKIP_SPLIT_FOL       : False\n\nEvaluating 1 tracker(s) on 1 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count\n\n\nEvaluating TrackByDetection\n\n    MotChallenge2DBox.get_raw_seq_data(TrackByDetection, OBJTR22-01)       0.1711 sec\n    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.2702 sec\n    HOTA.eval_sequence()                                                   0.3073 sec\n    Count.eval_sequence()                                                  0.0000 sec\n1 eval_sequence(OBJTR22-01, TrackByDetection)                            0.7511 sec\n\nAll sequences for TrackByDetection finished in 0.75 seconds\n\nHOTA: TrackByDetection-pedestrian  HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)\nOBJTR22-01                         39.458    48.459    32.238    67.952    53.977    39.871    49.078    77.514    46.798    55.504    71.343    39.598\nCOMBINED                           39.458    48.459    32.238    67.952    53.977    39.871    49.078    77.514    46.798    55.504    71.343    39.598\n\nCount: TrackByDetection-pedestrian Dets      GT_Dets   IDs       GT_IDs\nOBJTR22-01                         5854      4650      83        19\nCOMBINED                           5854      4650      83        19\n\nTiming analysis:\nMotChallenge2DBox.get_raw_seq_data                                     0.1711 sec\nMotChallenge2DBox.get_preprocessed_seq_data                            0.2702 sec\nHOTA.eval_sequence                                                     0.3073 sec\nCount.eval_sequence                                                    0.0000 sec\neval_sequence                                                          0.7511 sec\nEvaluator.evaluate                                                     1.3774 sec\nObserve that there are improvements to tracking throughout! Here is the evaluation summary with percentage improvements shown from the baseline tracker. Biggest gain is in association accuracy which makes sense since the detector based approach is using an optimization approach to assign object continuations between frames minimizing the overall cost of assignments. The baseline approach is doing this assignment without using a greedy approach by selecting the highest overlapping bounding box across frames.\n\n\n\nMetric\nScore\n\n\n\n\nHOTA\n39.45 (\\(\\uparrow\\) 14%)\n\n\nLocA\n77.514 (\\(\\downarrow\\) 0.5%)\n\n\nDetA\n48.459 (\\(\\uparrow\\) 4%)\n\n\nAssA\n32.238 (\\(\\uparrow\\) 26%)\n\n\n\n\nHOTA metrics for tracking by detection approach."
  },
  {
    "objectID": "posts/understanding-object-tracking-a-hands-on-approach/index.html#footnotes",
    "href": "posts/understanding-object-tracking-a-hands-on-approach/index.html#footnotes",
    "title": "Understanding Object Tracking: A Hands-on Approach, Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nObject Detection Papers with Code‚Ü©Ô∏é\nObject Detection and Tracking in 2020‚Ü©Ô∏é\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é\nRedmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.‚Ü©Ô∏é\nYOLO: Real-Time Object Detection‚Ü©Ô∏é\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é\nObject Detection and Tracking in 2020‚Ü©Ô∏é\nYin, Fei & Makris, Dimitrios & Velastin, Sergio. (2007). Performance evaluation of object tracking algorithms. 10th IEEE International Workshop on Performance Evaluation of Tracking and Surveillance (PETS2007).‚Ü©Ô∏é\nLuiten, J., OsÃÜep, A., Dendorfer, P. et al.¬†HOTA: A Higher Order Metric for Evaluating Multi-object Tracking. Int J Comput Vis 129, 548‚Äì578 (2021). https://doi.org/10.1007/s11263-020-01375-2‚Ü©Ô∏é\nMultiple object Tracking Benchmark‚Ü©Ô∏é\nJonathon Luiten, Arne Hoffhues, TrackEval‚Ü©Ô∏é\nJonathon Luiten, A.O. & Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.‚Ü©Ô∏é\nHow to evaluate tracking with the HOTA metrics‚Ü©Ô∏é\nHow to evaluate tracking with the HOTA metrics‚Ü©Ô∏é\nHow to evaluate tracking with the HOTA metrics‚Ü©Ô∏é\nJonathon Luiten, A.O. & Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.‚Ü©Ô∏é\nMOT Challenge Metrics Implementation‚Ü©Ô∏é\nHOTA Metric Implementation in Python‚Ü©Ô∏é\nTracking Evaluation on Your Own Data‚Ü©Ô∏é\nMOT Challenge Metrics Implementation‚Ü©Ô∏é\nTracking Evaluation on Your Own Data‚Ü©Ô∏é\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é\nHow to evaluate tracking with the HOTA metrics‚Ü©Ô∏é\nJonathon Luiten, A.O. & Leibe, B. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. International Journal of Computer Vision, 2020.‚Ü©Ô∏é\nRedmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.‚Ü©Ô∏é\nRedmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.‚Ü©Ô∏é\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html",
    "href": "posts/slam-with-crazyflie/index.html",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "",
    "text": "Perception, Planning, and Control are some of the high-level tasks to be performed by a successful autonomous agent. The act of perceiving the environment state such as obstacles and their locations is a perception task. Perception is a necessary skill for any autonomous agent. In the previous post on Motion Planning, we hand-coded the map of an obstacle course and used it for planning safe paths for the Crazyflie. However, this approach doesn‚Äôt scale when the environment gets large and dynamic. Let‚Äôs explore the task of environmental perception to build a realistic map of the environment."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#estimated-position-and-attitude",
    "href": "posts/slam-with-crazyflie/index.html#estimated-position-and-attitude",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Estimated position and attitude",
    "text": "Estimated position and attitude\nWhen a UAV is navigating in a space where there is no external support for localization such as GPS or cameras, only way to find the position of the UAV at any time is to estimate its position using onboard sensors and control commands issued to the UAV. Crazyflie does this estimation in it‚Äôs firmware and we can enjoy the estimated positions for free :) Using the python client API of Crazyflie, we can subscribe to position, attitude, and range observations using a callback mechanism. Position observations consists of x, y, and z coordinates estimated using optical flow (x and y coordinate) and a laser range finder (z coordinate). Attitude estimates consists of roll, pitch, and yaw of the UAV. We will use these estimates for creating a map of the environment."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#range-sensor-data",
    "href": "posts/slam-with-crazyflie/index.html#range-sensor-data",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Range sensor data",
    "text": "Range sensor data\nAs noted earlier, laser range sensors provide distance to obstacles in five directions. Laser range sensors used on Crazyflie is said to have 4 meters range, i.e., obstacles within this range will be detected. However, using a smaller range (e.g., 2 meters) will result in a reliable detection of obstacles. Since the battery blocks the laser range sensor facing in the up direction, we cannot use this observation (we probably don‚Äôt need this if we are working with 2D map like we will do in this post). For rest of the range sensor observations, we will have to transform them from body frame to global frame. This transformation requires that we have an estimated attitude of the vehicle, i.e., roll, pitch, and yaw of Crazyflie and estimated position, i.e., x, y, and z coordinate estimate of the Crazyflie."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#grid-representation",
    "href": "posts/slam-with-crazyflie/index.html#grid-representation",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Grid representation",
    "text": "Grid representation\nModels the robot environment as fixed sized grids where each grid‚Äôs occupancy is represented as a binary random variable. Note that the number of binary random variable we need to represent the map is equal to the number of cells in the grid. The binary random variable captures the event of a cell being occupied or not-occupied."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#continuous-representation",
    "href": "posts/slam-with-crazyflie/index.html#continuous-representation",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Continuous representation",
    "text": "Continuous representation\nIn a cluttered environment, using lines to describe smaller objects and keeping them separate from larger objects may be a challenging task. In other words, continuous representation introduces unnecessary complexity such as guessing polygon shapes of obstacles from range sensor data. A grid based representation addresses this concern using a probabilistic occupancy representation for each cell in the grid."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#occupancy-grids-implementation",
    "href": "posts/slam-with-crazyflie/index.html#occupancy-grids-implementation",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Occupancy grids implementation",
    "text": "Occupancy grids implementation\nCrazyflie has five laser range sensors on the multi-ranger deck. For a laser range sensor, here is an example of reliability measurements 1. As you can see that at 2000 millimeters i.e., 2 meters from the sensor, the standard deviation of measurement in around 25 millimeters i.e., 0.025 meters (2.5 cm). Beyond 2 meters from the sensor, the standard deviation gets larger and the data may be unusable for our scenario. Hence, we fix the range of usable laser range observations to less than or equal to 2 meters.\n\n\n\nReliability of laser range sensor with change in distance from the sensor\n\n\n\nInitialization\nWe represent the environment with grids of 0.1 meters \\(\\times\\) 0.1 meters. We represent each grid‚Äôs occupancy with a single number which is the probability of the grid being occupied (by an obstacle). In the implementation, we define a 500 \\(\\times\\) 500 grid each entry representing occupancy of 0.1 \\(\\times\\) 0.1 meters cell. Total size of the occupancy map in meters is (500 cells _ 0.1 meters per cell \\(\\times\\) 500 cells _ 0.1 meters per cell) which is (50 meters \\(\\times\\) 50 meters).\nWe assume that each cell is equally likely to be either occupied or not-occupied. This is achieved by assigning a probability of 0.5 for each cell while initializing the 500 \\(\\times\\) 500 numpy array.\n\n\nNumerical stability considerations\nWhen combining probabilities by repeated multiplication, we will probably encounter numerical underflow. One way to mitigate this would be use log-odds representation and update the log-odds as we make new observations of occupancy. If p is the probability of an event occurring then log-odds is defined as \\(\\frac{p}{1 - log(p)}\\).\n\n\nLog-odds vs.¬†raw sensor data\nIn a grid representation defined above, each cell represents a physical space. This space can be either occupied or unoccupied in the physical world. We need to use sensor data to estimate the true state of the cell space, when using Crazyflie, laser range sensors are used to estimate cell occupancies.\nWe have a choice of using raw sensor data as-is for determining cell occupancy. Why do we need probabilities? Here is a comparison of raw sensor data vs.¬†log-odds of occupancy probability used to update an occupancy grid. Note the noisy nature of the occupancy grid created using just the raw observations. The log-odds based representation on the right is much more robust to sensor noise.\n\n\n\nComparing raw sensor data vs.¬†log-odds of occupancy probabilities to update an occupancy grid.\n\n\n\n\nUpdate\nUpdating the log odds value at time t, \\(l_{t}\\) involves an addition \\(l_{t-1} + log(p / (1 - p))\\) where \\(p\\) is the probably of the observed event and \\(l_{t-1}\\) is the previous log odds value. For example, if we are updating the occupancy map, and sensor reports an obstacle, then \\(p\\) is the probability of the obstacle being present at the observed location. At first, this may seem strange and you may wonder is sensor reporting an obstacle is not good enough to say there is an obstacle for sure? Yes, there is an obstacle but we cannot completely discount the fact that sensors are sometimes noisy resulting in some uncertainty in the reported observations. Hence, if the sensor reports an obstacle, then we can be say there is a 90% chance that there is really an obstacle. This choice of 90% is arbitrary and you can essentially plug in any value that quantifies the uncertainty in sensor observations."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#mapping-software-architecture",
    "href": "posts/slam-with-crazyflie/index.html#mapping-software-architecture",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Mapping software architecture",
    "text": "Mapping software architecture\nCrazyflie perceives obstacles in the environment using the on-board laser range sensors on the multi-ranger deck. The configuration explained in the blog post enables Crazyflie to sense obstacles in the front, right, back, and left. The maximum trustable range is set to two meters, i.e., we update the occupancy map only with observations that are within the two meters range for a reliable map of the environment. The overall architecture of the mapping software and hardware is shown below.\n\n\n\nArchitecture of the mapping software components which include CrazyflieState, Navigator, and Mapper and interactions with the Crazyflie.\n\n\nCrazyflie radio is the communication link between the flight computer (PC or laptop) and the flight controller (on-board Crazyflie). Flight computer is responsible for higher level perception and planning of Crazyflie behaviors. Flight controller is responsible for lower level control of Crazyflie such as computing thrusts for the four motors to achieve a certain desired motion.\nObservations from on-board sensors of Crazyflie is abstracted as ‚Äústate‚Äù of Crazyflie with CrazyflieState. CrazyflieState subscribes to sensor updates from Crazyflie sensors and the state is updated as the sensor observations arrive.\nMapper initializes the occupancy map of the environment and utilizes CrazyflieState to update the occupancy map. Mapper exposes Crazyflie state to the Navigator."
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#evaluation",
    "href": "posts/slam-with-crazyflie/index.html#evaluation",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Evaluation",
    "text": "Evaluation\nLet‚Äôs now utilize the above mapping software and the Crazyflie to map the environment.\n\nObstacle course (controlled environment)\nThis is a setting that is staged with some obstacles and is much more controlled compared to real-world environments. This is a good starting point to test our mapping and navigation system.\n\n\n\nCrazyflie navigating an obstacle course using multi-ranger deck containing laser range sensors. Environment is mapped using SLAM where localization is done on Crazyflie (firmware) and mapping done on flight computer connected to the Crazyflie.\n\n\n\nCrazyflie navigating an obstacle course using multi-ranger deck containing laser range sensors. Environment is mapped using SLAM where localization is done on Crazyflie (firmware) and mapping done on flight computer connected to the Crazyflie.\n\nAs you can see, Crazyflie navigates the environment without flying into any of the obstacles in the environment. The updated occupancy map after the complete flight is shown below. The cyan color are the un-explored cells, yellow color indicates obstacle, and dark blue cells indicate free space on the map.\n\n\n\nOccupancy map created by autonomous navigation of an obstacle course which has less clutter\n\n\n\n\nIndoor environment with less clutter\nOur next test flight is in a slightly more realistic environment ‚Äì indoor environment inside a house. Crazyflie was able to navigate the space and create an occupancy map as shown below. The long areas are the corridors within the house.\n\n\n\nOccupancy map created by autonomous navigation of an indoor space. The green color indicates uncertainty, dark blue indicates un-occupied cells, and yellow indicates occupied cells\n\n\n\n\nIndoor environment with clutter (realistic)\nFinally, a more realistic environment with lot of clutter and obstacles that are almost undetectable by laser ranger sensors on Crazyflie. For example, observe the Crazyflie as it gets closer to the chair ‚Äì since the obstacle goes undetected, Crazyflie collides with the chair.\n\n\n\nWhile navigating a realistic indoor environment with clutter, Crazyflie crashes due to it‚Äôs inability to perceive objects in the environment\n\n\nIn realistic navigation scenarios, lighting conditions may not be ideal. Crazyflie relies on downward facing camera to estimate lateral motion. If the lighting conditions are not good or the intensity of lighting changes on the floor, Crazyflie may be misguided in knowing its actual lateral position. As you can see below, the Crazyflie transitions from stable lateral positioning to completely unstable lateral positioning resulting in a crash.\n\n\n\nWhile optical flow is a reliable way of estimating x and y displacements in well lit environment, some failures like this may lead to unpredictable behavior and eventual crash!"
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#conclusion",
    "href": "posts/slam-with-crazyflie/index.html#conclusion",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Conclusion",
    "text": "Conclusion\nMapping the environment with Crazyflie is a great learning experience. You will understand the basics of Crazyflie control, representation of obstacles in the environment, occupancy map and its updates, and finally navigating the environment when there are obstacles encountered by the Crazyflie.\nYou will notice that laser range sensors are quite reliable in uncluttered environments. It‚Äôs quite challenging to spot obstacles without any additional sophistication in spotting tricky obstacles such as chairs. One possible approach to address this limitation is to use senors that can detect even smaller obstacles, e.g., a camera can be used to map the environment and obstacles. This opens exciting opportunities such as object detection and recognition enabling Crazyflie to achieve more sophisticated tasks.\nComplete code for Crazyflie mapping and navigation can be found here"
  },
  {
    "objectID": "posts/slam-with-crazyflie/index.html#footnotes",
    "href": "posts/slam-with-crazyflie/index.html#footnotes",
    "title": "Simultaneous Localization and Mapping (SLAM) with Crazyflie",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJose A. Castellanos and Juan D. Tardos. 2000. Mobile Robot Localization and Map Building: A Multisensor Fusion Approach. Kluwer Academic Publishers, USA.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html",
    "href": "posts/backyard-flyer-project/index.html",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "",
    "text": "Udacity Flying Car and Autonomous Flight Engineer (FCND) nanodegree starts out with basics of autonomous flight and provides a broad overview of Unmanned Aerial Vehicles (UAVs) and their history. The first project called the Backyard Flyer is designed mostly to understand ways to interact with the simulator though event-driven python code. Event-driven programming deals with asynchronous nature of a drone where the state of the system changes over time and our code should respond to these state changes as they occur.\nFCND simulator is a valuable tool used to control and fly a drone in a virtual 3D world. Simulations are critical for UAVs/robotics as they provide a no-risk environment to develop and test out algorithms before deploying them on hardware. Once deployed on hardware, we run the risk of damaging the done or even worse risking the safety of people around the hardware.\nFor a complete description of the project and the solution, you can refer to the code here. Here is the outcome of the drone flying a predetermined square path on the simulator.\nIn this post, I will focus on porting this project to work with Crazyflie. Udacidrone API abstraction used to develop this project to work with the simulator supports Crazyflie as one of the platforms. Udacidrone API provides a protocol agnostic API to control a drone in the FCND simulator or other supported hardware such as PX4 powered drone or Crazyflie. So, any code written to work with the simulator should also work for Crazyflie."
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#udacidrone-and-associated-tools",
    "href": "posts/backyard-flyer-project/index.html#udacidrone-and-associated-tools",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Udacidrone and associated tools",
    "text": "Udacidrone and associated tools\nFor a detailed description of Udacidrone, you can refer to their getting started guide. Here, I will emphasize on essential steps to get basic setup working for communication with Crazyflie."
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#software",
    "href": "posts/backyard-flyer-project/index.html#software",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Software",
    "text": "Software\nClone the repository\ngit clone https://github.com/pramodatre/FCND-projects-crazyflie-port.git\nCreate a virtual environment and activate\nThis project is tested to work with python 3.6.8 and I recommend using pyenv to manage your python versions if you are using MacOS.\npython3 -m venv fcnd\nsource fcnd/bin/activate\nInstall all dependencies\npip install git+https://github.com/udacity/udacidrone.git"
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#hardware",
    "href": "posts/backyard-flyer-project/index.html#hardware",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Hardware",
    "text": "Hardware\nIt‚Äôs assumed that you have followed instructions on setting up Crazyflie2.1. Ensure you have Crazyflie2.1 PA radio plugged-in and the Crazyflie2.1 is turned ON."
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#configuration",
    "href": "posts/backyard-flyer-project/index.html#configuration",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Configuration",
    "text": "Configuration\nLaunch cfclient (while the virtual environment is activated). You will see the cfclient UI where you can search and connect to your Crazyflie.\n\n\n\nConnect to Crazyflie using cfclient UI\n\n\nOnce connected, it is recommended to change the connection bandwidth from 250k to 2M.\n\n\n\nChange bandwidth of Crazyflie connection to recommended 2M\n\n\n\nChange bandwidth of Crazyflie connection to recommended 2M\n\n\nProgramming paradigm\nWhen writing software using Udacidrone, event driven programming paradigm is leveraged for its ability to capture asynchronous operations. For example, if we would like the drone to take action depending on its position, we would first create a listener to position observations. When position observations are made the position listener is invoked. The position listener can read the current position and decide on its action. One such example in this project is we check if current altitude is ‚Äúclose‚Äù to the desired altitude. If so, we start navigation to waypoints."
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#simulator-to-crazyflie",
    "href": "posts/backyard-flyer-project/index.html#simulator-to-crazyflie",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Simulator to Crazyflie",
    "text": "Simulator to Crazyflie\nThese instructions were provided as part of the porting instructions in the project. However, for others, who may not have access to the course content, here is the project that works with the simulator. To port backyard_flyer.py to Crazyflie, following modifications are to be implemented:\n\nUpdate connection parameters\nAdapt code to work no state callbacks with Crazyfile for takeoff and landing\nUpdate waypoints that is safe indoor space by reducing the box size\nUpdate takeoff altitude to something that works indoor\nUpdate waypoint acceptance thresholds to match new box size\n\nLaunch crazyflie_backyard_flyer.py script with the PA radio plugged in to your laptop and Crazyflie switched ON to see it fly a rectangular trajectory.\n&gt; python crazyflie_backyard_flyer.py\nLogs/TLog.txt\nCreating log file\nLogs/NavLog.txt\nstarting connection\nConnecting to radio://0/80/2M\nConnected to radio://0/80/2M\nWaiting for estimator to find position...\nClosing log file\ntakeoff transition\nfilter has converge, position is good!\nwaypoint transition\nwaypoint to navigate to: [0.75 0.   0.5 ]\n0.75 0.0 0.5\nthe delay time for the move command: 2.7578634101417574\nvel vector: (0.19966524161618923, -0.01156681850591479, 0)\nwaypoint transition\nwaypoint to navigate to: [0.75 0.75 0.5 ]\n0.75 0.75 0.5\nthe delay time for the move command: 4.019526425683314\nvel vector: (0.03779849779012415, -0.19639570658446173, 0)\nwaypoint transition\nwaypoint to navigate to: [0.   0.75 0.5 ]\n0.0 0.75 0.5\nthe delay time for the move command: 4.781339464967377\nvel vector: (-0.19740196558339365, -0.032131977589508295, 0)\nwaypoint transition\nwaypoint to navigate to: [0.  0.  0.5]\n0.0 0.0 0.5\nthe delay time for the move command: 4.743249739164866\nvel vector: (-0.03620927863315014, 0.19669491132428135, 0)\nwaypoint transition\nlanding transition\nmanual transition"
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#crazyflie-in-action",
    "href": "posts/backyard-flyer-project/index.html#crazyflie-in-action",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Crazyflie in action",
    "text": "Crazyflie in action\n\n\n\nCrazyflie flying a square trajectory of side 0.75 meters"
  },
  {
    "objectID": "posts/backyard-flyer-project/index.html#conclusion",
    "href": "posts/backyard-flyer-project/index.html#conclusion",
    "title": "Porting Backyard Flyer Project to Crazyflie",
    "section": "Conclusion",
    "text": "Conclusion\nUdacidrone API allows us to write software once and deploy to work with multiple target environments such as FCND simulator, Crazyflie, or any PX4 drone. The tooling is quite mature and thanks to the documentation of Udacidrone API, the project instructions, and Crazyflie setup documentation ‚Äì you can start making your ideas fly relatively quickly."
  },
  {
    "objectID": "posts/build-tf-from-source-on-mac/index.html",
    "href": "posts/build-tf-from-source-on-mac/index.html",
    "title": "Building Tensorflow from Source on MacOS",
    "section": "",
    "text": "Tensorflow is a popular choice if you are working on Computer Vision projects such as image classification, object detection, pose estimation, or just any Machine Learning task. I have a MacBook Pro and while using tensorflow, I noticed a warning when importing tensorflow which stated the following.\n2020-05-08 19:57:50.106998: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2020-05-08 19:57:50.128346: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7febdd364b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-05-08 19:57:50.128370: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\nThis warning is issued because the tensorflow binary installed using pip install tensorflow was not built specifically for my machine. Since the binary should work on a wide varity of devices, the tensorflow binary on pip repository would be built such that it works on majority of the CPUs. Using this generic binary prevents tensorflow from using hardware specific optimizations. These optimizations lead to gain in performance which is especially important when using CPU-only version tensorflow. Since I use CPU-only configuration for tensorflow on my MacBook Pro (I do not have NVIDIA GPUs), this performance gain (up to 300% as mentioned here) is very much welcome!\nIf you observe the warning closely, you see TensorFlow binary was not compiled to use: AVX2 FMA. AVX (Advanced Vector Extensions) are instruction set extensions and specifically, FMA (Fused Multiply Accumulate) introduced by AVX speeds up linear algebra computation. For more information, you can read the stackoverflow post here.\nMotivated by 300% gain I could get, I started following instructions for building tensorflow from source using instructions from official tensorflow website and stackoverflow post.\nBefore you build the code, if your MacOS version is &lt; 10.14, I would recommend to upgrade it to 10.14 before you proceed. I lost lot of time since I initially started with 10.13 MacOS version and Python 3.7 version and finally could not install the *.whl file using the pip command (this is the last step which will be outlined later). Note that building tensorflow from source is time consuming and memory intensive.\nI followed these instructions to build tensorflow from source: https://www.tensorflow.org/install/source\nSome issues and resolutions that worked for me: * I had to explicit select Xcode using sudo xcode-select -s /Applications/Xcode.app/Contents/Developer otherwise, I got compile errors when running bazel build //tensorflow/tools/pip_package:build_pip_package * Having issues as I‚Äôm unable to install using pip install /tmp/tensorflow_pkg/tensorflow-version-tags.whl. I‚Äôm getting the following error\n(tf-build) Pramods-MacBook-Pro:tensorflow pramodanantharam$ pip install /tmp/tensorflow_pkg/tensorflow-2.2.0-cp37-cp37m-macosx_10_14_x86_64.whl\nERROR: tensorflow-2.2.0-cp37-cp37m-macosx_10_14_x86_64.whl is not a supported wheel on this platform.\n\nTo fix this, I upgraded my MacOS from 10.13 to 10.14 which resolved the issue.\n\n\nConclusion\n\nBuilding tensorflow from source is a good idea if you do not have GPU on your machine ‚Äì if you have a GPU version of tensorflow, you need not bother for this marginal performance gain.\nHaving MacOS 10.14.x was necessary for me to complete the process of installing"
  },
  {
    "objectID": "posts/motion-planning-project/index.html",
    "href": "posts/motion-planning-project/index.html",
    "title": "Porting Motion Planning Project to Crazyflie",
    "section": "",
    "text": "Motion Planning essentially answers the question How to get from point A to point B? given the map of the environment. In this project on Udacity, the drone had to navigate through an urban environment with tall buildings to reach the destination specified as GPS coordinates. You can checkout the Udacity project along with my solution to the project here. In this blog post, we will port the Udacity project to Crazyflie, a micro-UAV that can be safely used indoors. We use local coordinates indoors derived from the flow deck for lateral position (x and y coordinate) and laser range sensor for drone altitude (z coordinate).\nWe assume that the map of the environment is given to us ‚Äì this is an assumption to simplify the problem and focus on understanding motion planning ideas. In reality, the map of environment may not be available or even if it‚Äôs available, it may not be precise. Some of the obstacles such as furniture, people, or other clutter in the indoor space may not be captured in the map. However, our assumption lets us deal with the problem in a manageable way and learn the basics of motion planning."
  },
  {
    "objectID": "posts/motion-planning-project/index.html#representation",
    "href": "posts/motion-planning-project/index.html#representation",
    "title": "Porting Motion Planning Project to Crazyflie",
    "section": "Representation",
    "text": "Representation\nWe need to represent the surroundings of the UAV in such a way that lets us store and perform queries related to the obstacles in the environment. Further, we need to represent the waypoints of the UAV from the start to the goal location. For a grid representation, waypoints may be a sequence of cells from start to goal location in a discrete space. A graph representation would have nodes representing waypoints with coordinates of each node in a continuous space.\n\nGrids\nWe can split the environment into grids and build an occupancy map representing various obstacles in the environment. Grids work well when the UAV is confined to a smaller space such as indoor environment but doesn‚Äôt scale well if we need to model a large area such as city. In a grid representation, each cell in the grid is either occupied or not occupied by an obstacle. We can find paths from point A to point B by traversing the cells that are not occupied from the starting point to the destination point. In the Udacity motion planning project, we start with a grid representation and later extend the solution to a graph representation for scalability (solution has to scale for navigation over multiple city blocks). When performing motion planning for Crazyflie indoors, a grid based representation should work fine.\n\n\nGraphs\nGraph representations are much more compact than grid representation where each node is a possible waypoint for the UAV. Each node is chosen such that there are no obstacles at the location of the node and along the line segments that connect these nodes. In the Udacity project, an occupancy map of the city is used to sample nodes and edges, and checked for collision with obstacles. You can imaging this kind of look-up, i.e., finding if a node is in free space and line segments connecting these nodes doesn‚Äôt cross any obstacles is quite compute intensive since there may be thousands of obstacles to process. You need to store the lateral span of an obstacle such as a building and it‚Äôs height and quickly able to answer the question if a node represented by coordinates \\((x_{1}, y_{1}, z_{1})\\) is away from obstacles (some buffer is introduced for accommodating the UAV size and additional safety measure, e.g., 5 meters). This project exemplifies the role of k-d trees in a real-world context, a data structure that is very much suitable for representation and query of points/lines and their intersection with obstacles.\nThere may be too many waypoints regardless of your representation (grid or graph), i.e., for a UAV too many waypoints means too many stop and starts and this results in a jerky motion. Instead, we need smooth flying of the UAV ‚Äì this is possible if we can create a high-level ‚Äúsummary‚Äù of the too many waypoints we have. We use Bresenham algorithm to condense waypoints to minimal points enabling UAV to fly without unnecessary stopping at extraneous waypoints.\nProbabilistic Road Map (PRM) scales well for large spaces compared to approaches such as Voronoi graphs or grid based approaches. However, with the FCND simulator, I had difficulty using PRM so ended up using Voronoi graphs and Bresenham for refinements of paths to come up with the final flight paths for the UAV. When we port this code to Crazyflie, we will use PRM instead of Voronoi graphs as it‚Äôs a more practical and scalable approach."
  },
  {
    "objectID": "posts/motion-planning-project/index.html#search",
    "href": "posts/motion-planning-project/index.html#search",
    "title": "Porting Motion Planning Project to Crazyflie",
    "section": "Search",
    "text": "Search\nThere may be multiple paths an UAV can choose to go from point A to point B. Planning is posed as a search problem over Grids or a Graph. A popular approach to search paths is \\(A^{*}\\) search which takes initial position, the goal position, possible actions at each step, and a heuristic function as inputs, and returns the optimal path which can be used by the drone for its navigation."
  },
  {
    "objectID": "posts/motion-planning-project/index.html#footnotes",
    "href": "posts/motion-planning-project/index.html#footnotes",
    "title": "Porting Motion Planning Project to Crazyflie",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChoset, H, Lynch, KM, Hutchinson, S, Kantor, G, Burgard, W, Kavraki, L & Thrun, S 2005, Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press.‚Ü©Ô∏é\nPython sample codes for robotics algorithms‚Ü©Ô∏é\nThe Open Motion Planning Library‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html",
    "href": "posts/sterio-vision-exploration/index.html",
    "title": "Disparity Map Computation in Python and C++",
    "section": "",
    "text": "Stereo vision is the term used for the process of inferring 3D depth information from 2D images 1. 2D images may be from a Stereo Rig, usually consisting of two cameras slightly displaced horizontally similar to our two eyes. In fact, stereopsis 2 takes inspiration from our ability to infer depth information using our eyes. For a deeper explanation of stereo vision, you can refer to 3 4. A comprehensive and clear explanation of Stereo Vision is presented here 5. If you would like to understand depth calculation clearly, you can refer to 6. In this post, I will walk you through the implementation steps in python and subsequent parallel implementation in C++."
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html#depth-estimation",
    "href": "posts/sterio-vision-exploration/index.html#depth-estimation",
    "title": "Disparity Map Computation in Python and C++",
    "section": "Depth Estimation",
    "text": "Depth Estimation\nBefore we proceed coding, I would like to explain how can we estimate depth using the disparity map? Without this motivation, I feel it is pointless to explain disparity map implementation. I had difficulty understanding a specific part of the derivation of depth equation which I will point out later. May be it‚Äôs just me, but, I thought of writing this up so that it may help others who may have similar question as I did. A simplified stereo setup with two cameras is shown here. It‚Äôs a bird‚Äôs eye view of the camera setup and a point \\(P\\) for which we are trying to estimate the depth/distance form the camera. To estimate depth from stereo implies that we need to estimate \\(Z\\) in the figure. \\(Z\\) is the distance of point \\(P\\) from the camera.\n\n\n\nFigure 1. Derivation of depth for a simplified stereo setting with two cameras with perfectly aligned centers and having same focal lengths. (source: CS 4495 Computer Vision ‚Äì A. Bobick)\n\n\nIn the above setup, let \\(C_{L}\\) be the camera on the left and \\(C_{R}\\) be the camera on the right. Both these cameras have the same focal length \\(f\\). Distance between camera centers is \\(B\\). A line from point \\(P\\) to the camera center of \\(C_{L}\\) intersects the image plane at \\(p_{l}\\). A line from point \\(P\\) to camera center of \\(C_{R}\\) intersects \\(C_{R}\\)‚Äôs image plane at \\(p_{r}\\). Note that the triangles \\(p_{l} P p_{r}\\) and \\(C_{L}PC_{R}\\) are similar triangles. Since these triangles are similar, their ratio of base to height should be the same, i.e., \\(\\frac{B}{Z}\\) = \\(\\frac{p_{l}p_{r}}{Z-f}\\). From the figure, we have \\(p_{l}p_{r}\\) to be \\(B - (x_{l}+x_{r})\\). However, in all the derivations in multiple references, \\(p_{l}p_{r}\\) is told to be \\(B - x_{l} + x_{r}\\) which totally confused me. It is quite clear from the figure, to get \\(p_{l}p_{r}\\) we need to subtract (\\(x_{l} + x_{r}\\)) from \\(B\\).\nLet‚Äôs say I would like to test my hypothesis that \\(p_{l}p_{r} = B - (x_{l}+x_{r})\\). When we use our depth sensing system in practice, we will have to feed in the focal length (\\(f\\)), base length (\\(B\\)), \\(x_{l}\\), and \\(x_{r}\\) to obtain \\(Z\\) which is the depth estimation for point \\(P\\). \\(x_{l}\\) is a positive value since it is to the right of the camera center line passing through the image plane which serves as the origin. Similarly, \\(x_{r}\\) is a negative number since it is to the left of the \\(C_{R}\\)‚Äôs center line which serves as the origin. Now, if we use the equation \\(p_{l}p_{r} = B - (x_{l}+x_{r})\\) with -ve value for \\(x_{r}\\) we end up adding \\(x_{r}\\) to \\(B\\) instead of subtracting, i.e., we will end up with \\(p_{l}p_{r} = B - (x_{l}-x_{r}) = B - x_{l} + x_{r}\\). This length is incorrect. Say, we used \\(p_{l}p_{r} = B - (x_{l}-x_{r})\\) and since we have \\(x_{r}\\) as negative, \\(p_{l}p_{r} = B - (x_{l}-(-x_{r})) = B - (x_{l}+x_{r})\\). This is the reason we have \\(-x_{r}\\) in the equation to estimate depth from stereo images. Now, a negative sign for \\(x_{r}\\) in the depth estimation equation makes sense to me. So, finally, to estimate depth, we can use the following equation \\(Z=f \\frac{B}{x_{l}-x_{r}}\\)."
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html#disparity-map",
    "href": "posts/sterio-vision-exploration/index.html#disparity-map",
    "title": "Disparity Map Computation in Python and C++",
    "section": "Disparity Map",
    "text": "Disparity Map\nDepth is inversely proportional to disparity, i.e., from the depth estimation equation, we have \\(Z \\propto \\frac{1}{x_{l}-x_{r}}\\). As disparity (\\(x_{l}-x_{r}\\)) increases, \\(Z\\) decreases and for lower disparity (\\(x_{l}-x_{r}\\)), we would have higher \\(Z\\). This is intuitive if you hold your index finger near your eyes and alternate seeing from your left and right eyes. You will notice that your finger jumps a lot in your view compared to other distant objects. We will use a technique called block matching to find the correspondences between pixels in the two images as outlined in 7. Here is a summary of steps we need for computing disparity map:\n\nInput: Left image and right image (from perfectly aligned cameras) of width \\(w\\) and height \\(h\\), block size in pixels, and search block size\nOutput: Disparity map of width \\(w\\) and height \\(h\\)\nWhy do we need to specify block size and search block size?\n\nFor every pixel in the left image, we need to find the corresponding pixel in the right image. Since pixel values may be noisy and is influenced by many factors such as sensor noise, lighting, mis-alignment, etc., we may have to rely on a group of surrounding pixels for comparison.\nBlock size refers to the neighborhood size we select to compare pixels from left image and the right image specified as number of pixels in height and width. An example block is shown as a white box in both left and right images in Figure 2.\nSearch block size refers to a rectangle (shown in black on the right image) in which we will search for best matching block. Notice that for the selected block from the left image, to get the corresponding image region, you will have to move the white smaller rectangle to the left in the black rectangle as shown in the third image in Figure 2.\n\n\n\n\n\nFigure 2. Block matching example. (Image source: Middlebury Stereo Datasets)\n\n\n\nFor a pixel in the left image, select the pixels in its neighborhood specified as block size from the left image.\nCompute similarity score by comparing each block from the left image (same size as block size) and each block selected from the search block in the right image. Slide block on the right image by one pixel within the search block (black rectangle). Record all the similarity scores.\nFind the highest pixel similarity score from the previous step. For the block with highest similarity, return the pixel location at the center of the block as the best matching pixel.\nIf \\(x_{l}\\) is the column index of the left pixel, and the highest similarity score was obtained for a block on the right image whose center pixel has column index \\(x_{r}\\), we will note the disparity value of \\(\\left\\|x_{l}-x_{r}\\right\\|\\) for the location of left image pixel.\nRepeat the matching process for each pixel in the left image and note all the disparity values for the left image pixel index.\nWe will start building the basic building blocks first and later combine these building blocks to compute disparity map.\n\n\nSimilarity metric\nWe need to define a notion of similarity between two blocks of pixels. Sum of absolute difference between pixel values is an intuitive metric for similarity. For example, the pair (3,5) is more similar compared to the pair (3,6) since the absolute difference between numbers in the first pair |3 - 5| &lt; difference between numbers in the second pair |3 - 6|, i.e., 2 &lt; 3. If there are multiple such values for comparison, we sum up the differences. Let‚Äôs implement sum of absolute difference method to be used for comparing blocks of pixels. We will loop over each row (\\(i\\)) and column (\\(j\\)) in both left and right blocks using \\(\\Sigma_{i,j} |B_{i,j}^{l} - B_{i,j}^{r}|\\). Pixel blocks with lower sum of absolute difference value are more similar than pixel blocks with higher sum of absolute difference value.\nimport numpy as np\n\ndef sum_of_abs_diff(pixel_vals_1, pixel_vals_2):\n    \"\"\"\n    Args:\n        pixel_vals_1 (numpy.ndarray): pixel block from left image\n        pixel_vals_2 (numpy.ndarray): pixel block from right image\n\n    Returns:\n        float: Sum of absolute difference between individual pixels\n    \"\"\"\n    if pixel_vals_1.shape != pixel_vals_2.shape:\n        return -1\n\n    return np.sum(abs(pixel_vals_1 - pixel_vals_2))\n\n\nBlock comparisons\nWe can compare a block of pixels from the left image with a block of pixels in the right image using the sum_of_abs_diff method we just defined. However, note that we need to compare a single block of pixels in the left image to multiple blocks of pixels on the right image (like we defined in Why do we need to specify block size and search block size?). These multiple blocks are to be selected within the search block shown as a black rectangle in Figure 2. We will slide the white box one pixel at a time starting from left most position within the black box to get candidate blocks for comparison from the right image. We note the block from the right image that has lowest sum of absolute difference score. The corresponding row and column index (y, x) is returned by our implementation here.\nBLOCK_SIZE = 7\nSEARCH_BLOCK_SIZE = 56\n\ndef compare_blocks(y, x, block_left, right_array, block_size=5):\n    \"\"\"\n    Compare left block of pixels with multiple blocks from the right\n    image using SEARCH_BLOCK_SIZE to constrain the search in the right\n    image.\n\n    Args:\n        y (int): row index of the left block\n        x (int): column index of the left block\n        block_left (numpy.ndarray): containing pixel values within the\n                    block selected from the left image\n        right_array (numpy.ndarray]): containing pixel values for the\n                     entrire right image\n        block_size (int, optional): Block of pixels width and height.\n                                    Defaults to 5.\n\n    Returns:\n        tuple: (y, x) row and column index of the best matching block\n                in the right image\n    \"\"\"\n    # Get search range for the right image\n    x_min = max(0, x - SEARCH_BLOCK_SIZE)\n    x_max = min(right_array.shape[1], x + SEARCH_BLOCK_SIZE)\n    #print(f'search bounding box: ({y, x_min}, ({y, x_max}))')\n    first = True\n    min_sad = None\n    min_index = None\n    for x in range(x_min, x_max):\n        block_right = right_array[y: y+block_size,\n                                  x: x+block_size]\n        sad = sum_of_abs_diff(block_left, block_right)\n        #print(f'sad: {sad}, {y, x}')\n        if first:\n            min_sad = sad\n            min_index = (y, x)\n            first = False\n        else:\n            if sad &lt; min_sad:\n                min_sad = sad\n                min_index = (y, x)\n\n    return min_index\n\n\nDisparity calculation\nWe can use the block comparison implementation to compute disparity values for every pixel between left and right images. As explained earlier, we will use a pixel blocks (neighboring pixels) to find pixel correspondences between left and right images. To do this, for a pixel at row \\(r\\) and column \\(c\\) from the left image, we select block of pixels \\(b_{r, c}\\) from the left image using BLOCK*SIZE parameter. We invoke block comparison for \\(b*{r, c}\\) and get back min_index containing row and column index of the best matching pixel from the right image, (y, x). For the pixel (r, c) of the left image, the best matching pixel from the right image is at (y, x). The disparity for pixel (r, c) is computed using \\(|c - x|\\). We need to compute disparity for each pixel of the left image and collect disparity values in a matrix of size width and height of the left/right image.\nh, w = input_image.shape\ndisparity_map = np.zeros((h, w))\nmin_index = compare_blocks(y, x, block_left, right_array, block_size=BLOCK_SIZE)\ndisparity_map[y, x] = abs(min_index[1] - x)\n\n\nDisparity map\nLet‚Äôs now setup a loop to go over each pixel in the left image, select a block of pixels from the left image, invoke compare_blocks, compute disparity values, and store disparity values in a matrix referred to as the disparity map. The indexing used here is quite lossy and a careful consideration may help in reducing loss of disparity values at image borders.\nfor y in tqdm(range(BLOCK_SIZE, h-BLOCK_SIZE)):\n        for x in range(BLOCK_SIZE, w-BLOCK_SIZE):\n            block_left = left_array[y:y + BLOCK_SIZE,\n                                    x:x + BLOCK_SIZE]\n            min_index = compare_blocks(y, x, block_left,\n                                       right_array,\n                                       block_size=BLOCK_SIZE)\n            disparity_map[y, x] = abs(min_index[1] - x)\nHere is a visualization of the disparity map computed for left and right images shown in Figure 3.\n\n\n\nFigure 3. Visualization of disparity values computed using left and right image of the scene shown in the first figure. Hot/lighter color indicates higher value of disparity and cooler/darker color indicates lower value of disparity\n\n\nWe use tqdm to show progress and note the time it takes to compute the disparity map in python. \nDisparity map computation using this python implementation took 2 minutes 37 seconds for left and right images of size (height=375, width=450). I was quite disappointed by this slow run-time for such a small image size and wondered about the practical use of my python implementation. There are many optimizations proposed in 8 and 9. The most obvious one for me is to use all the compute power on my machine. Since python cannot take advantage of all the cores on my machine, I was motivated to use C++ to parallelize the disparity map computation."
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html#parallel-implementation-in-c",
    "href": "posts/sterio-vision-exploration/index.html#parallel-implementation-in-c",
    "title": "Disparity Map Computation in Python and C++",
    "section": "Parallel implementation in C++",
    "text": "Parallel implementation in C++\nComparing pixel blocks from the left image (white box) with the pixel blocks from the right image can be done in parallel. A naive approach may be to create a thread pool and assign a single block comparison to a thread. Later, after block comparison for each pixel in the left image, we accumulate all the results into a disparity map. One concern is the cost of spawning a thread and later aggregating all the results which probably impedes the benefits of parallelizing this computation.\nA slightly practical approach to parallelize this computation is to find the number of cores on the machine where you would compute the disparity map and split the computation accordingly. We can split disparity map computation into \\(n\\) chunks where \\(n\\) is the number of cores on your machine. Think of the left image and right image as 2D space split into \\(n\\) horizontal strips. We will compute disparity map for each strip pair (one from left image and another from right image) in parallel. Later, we will combine the \\(n\\) disparity maps into a single disparity map. Since this approach leverages all the cores on the machine, we should see a gain in performance.\n\nSimilarity metric and block comparisons\nLet‚Äôs start with some basic methods we need. First, we need a method to compare blocks selected from left and right images. We will write a general method that can do block comparison for a given pixel location and pixel block width and height. Method compare_blocks implemented here returns the disparity value for a specified pixel index (row, col) and block size (width, height).\nint compare_blocks(const int row, const int col, const int width, const int height, const Mat *left_img, const Mat *right_img)\n{\n    int sad = 0;\n    int min_col = col;\n    // compute bounding box for left image with (row, col) as top left point\n    // compute bottom right point using (row, col)\n    int bottom_row = min(row + BLOCK_SIZE, height - 1); // zero indexed, hence using (height - 1)\n    int bottom_col = min(col + BLOCK_SIZE, width - 1);\n    // compute bounding box for right image block in which\n    // we will scan and compare left block\n    int col_min = max(0, col - SEARCH_BLOCK_SIZE);\n    int col_max = min(width, col + SEARCH_BLOCK_SIZE);\n    bool first_block = true;\n    int min_sad = 0;\n    for (int r_indx = col_min; r_indx &lt; col_max; ++r_indx)\n    {\n        sad = 0;\n        for (int i = row; i &lt; bottom_row; ++i)\n        {\n            int r_img_col = r_indx;\n            for (int j = col; j &lt; bottom_col; ++j)\n            {\n                Scalar left_pixel = left_img-&gt;at&lt;uchar&gt;(i, j);\n                // Right image index should be updated using offset\n                // since we need to scan both left and right of the\n                // block from the left image\n                Scalar right_pixel = right_img-&gt;at&lt;uchar&gt;(i, r_img_col);\n                sad += abs(left_pixel.val[0] - right_pixel.val[0]);\n                ++r_img_col;\n            }\n        }\n\n        if(first_block)\n        {\n            min_sad = sad;\n            min_col = r_indx;\n            first_block = false;\n        }\n        else\n        {\n            if(sad &lt; min_sad)\n            {\n                min_sad = sad;\n                min_col = r_indx;\n            }\n        }\n    }\n    //cout &lt;&lt; \"min sad: \" &lt;&lt; min_sad &lt;&lt; \" \";\n\n    return col - min_col;\n}\n\n\nDisparity calculation\nWe can invoke block comparisons for each pixel in the left image using the following set-up. Here, the start and end rows and columns of the chunks are passed as arguments. I will next describe the derivation of these indexes.\nvoid compute_disparity(int start_chunk_row, int end_chunk_row, int start_chunk_col, int end_chunk_col, Mat *left_img, Mat *right_img, Mat *disparity_map)\n{\n    int height = left_img-&gt;rows;\n    int width = left_img-&gt;cols;\n    for (int i = start_chunk_row; i &lt; end_chunk_row; ++i)\n    {\n        for (int j = start_chunk_col; j &lt; end_chunk_col; ++j)\n        {\n            int disp = compare_blocks(i, j, height, width, left_img, right_img);\n            if(disp &lt; 0)\n            {\n                mtx.lock();\n                disparity_map-&gt;at&lt;uchar&gt;(i, j) = 0;\n                mtx.unlock();\n            }\n            else\n            {\n                mtx.lock();\n                disparity_map-&gt;at&lt;uchar&gt;(i, j) = disp;\n                mtx.unlock();\n            }\n        }\n    }\n}\n\n\nChunking\nTo chunk disparity map computation, we need to find the indices for each chunk we want to process. We will implement the method get_chunk_indices which takes maximum range of the index and number of chunks to produce. We will return the chunk indices.\nvector&lt;int&gt; get_chunk_indices(int max, int num_chunks)\n{\n    vector&lt;int&gt; chunks;\n    int step = max / num_chunks;\n    for (int i = 0; i &lt; max; i = i + step)\n    {\n        chunks.push_back(i);\n    }\n    chunks[chunks.size() - 1] = max - 1;\n\n    return chunks;\n}\n\n\nSpawning threads\nWe will spawn specified number of threads and on each thread we will compute disparity values for a single chunk which contains left and right image strips for which we are computing the disparity values. Each thread populates the disparity map and when all the threads finish, disparity_map will contain the final disparity map.\nstatic const int num_threads = 8;\nvector&lt;int&gt; height_chunks = get_chunk_indices(height, num_threads);\nfor (int i = 0; i &lt; height_chunks.size() - 1; ++i)\n{\n    t[i] = thread(compute_disparity, height_chunks[i], height_chunks[i + 1], 0,\n    width - 1, left_img, right_img, &disparity_map);\n}\nHere is the console output for the parallelized computation of disparity map. Complete disparity map calculation took just 6 seconds! Just to recap, with the python implementation, disparity map calculation took 2 minutes, 37 seconds. Using this C++ implementation, we get 26X gain in speed for computing the disparity map.\n$ g++ $(pkg-config --cflags --libs opencv4) -std=c++11 stereo_vision_parallel.cpp -o stereo_vision\n$ ./stereo_vision data/left.png data/right.png\nLeft image shape: [450 x 375]\nRight image shape: [450 x 375]\nInitial disparity map: [450 x 375]\nTotal entries in disparity map: 168750\nUsing 8 threads for computation...\nExecution time: 6 seconds (0.1) mins\nHere the visualization of the disparity map calculated using the parallel implementation."
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html#conclusion",
    "href": "posts/sterio-vision-exploration/index.html#conclusion",
    "title": "Disparity Map Computation in Python and C++",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to implement the basic idea of Stereo Vision to compute disparity values. Hope you enjoyed translating Stereo Vision ideas to working code! Without any optimizations, the python implementation is too slow for any practical use. With parallel implementation, we reduced the running time for 375 by 450 pixels image form 2 minutes 37 seconds to just 6 seconds. Even though this is a significant jump in performance, it is far from practical use for any real-time systems that rely on stereo to estimate its environment. For example, a robot using vision guided navigation cannot afford to spend 6 seconds for processing two frames (one from left and and another from right camera). A far more optimized computation is necessary especially for real-time consumption of depth information.\nAll the source code presented in this post can be found here"
  },
  {
    "objectID": "posts/sterio-vision-exploration/index.html#footnotes",
    "href": "posts/sterio-vision-exploration/index.html#footnotes",
    "title": "Disparity Map Computation in Python and C++",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é\nForsyth, D., & Ponce, J. (2003). Computer vision: A modern approach. Upper Saddle River, N.J: Prentice Hall.‚Ü©Ô∏é\nhttp://mccormickml.com/2014/01/10/stereo-vision-tutorial-part-i/‚Ü©Ô∏é\nhttp://mccormickml.com/assets/StereoVision/Stereo%20Vision%20-%20Mathworks%20Example%20Article.pdf‚Ü©Ô∏é\nhttp://vision.deis.unibo.it/~smatt/Seminars/StereoVision.pdf‚Ü©Ô∏é\nhttps://www.cc.gatech.edu/~afb/classes/CS4495-Fall2013/slides/CS4495-06-Stereo.pdf‚Ü©Ô∏é\nhttp://mccormickml.com/2014/01/10/stereo-vision-tutorial-part-i/‚Ü©Ô∏é\nhttp://mccormickml.com/2014/01/10/stereo-vision-tutorial-part-i/‚Ü©Ô∏é\nhttp://mccormickml.com/assets/StereoVision/Stereo%20Vision%20-%20Mathworks%20Example%20Article.pdf‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/usb-camera-and-audio-streaming-pi/index.html",
    "href": "posts/usb-camera-and-audio-streaming-pi/index.html",
    "title": "Live Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone",
    "section": "",
    "text": "There may be many applications such as wildlife monitoring, baby monitoring, or in general security system applications where you may need to monitor both video and audio stream. One such example of using audio is the Rainforest Connection Species Audio Detection challenge on Kaggle. I wanted to use Raspberry Pi to accomplish video + audio streaming using USB camera and a USB microphone. I have this special requirement since I did not want to buy a Pi Camera as I already have a good quality USB camera and just wanted to use this. Most of the references I was able to find used Raspberry Pi Camera and it was quite confusing for me to navigate this space.\nAfter a bit of struggle to find the relevant information I needed, I got both audio and video streaming working with Raspberry Pi + USB camera + USB microphone. This post is to summarize my findings so that it may be helpful for someone with similar requirements.\nThere may be many approaches but the most straightforward one for me was to use UV4L streaming server, specifically, the two-way audio/video intercom recorder, WebRTC. However, for my purpose, I only need one way stream from USB camera and microphone connected to the Raspberry Pi. WebRTC provides a web UI you can use to start or end the stream from the server."
  },
  {
    "objectID": "posts/usb-camera-and-audio-streaming-pi/index.html#step-1-install-uv4l",
    "href": "posts/usb-camera-and-audio-streaming-pi/index.html#step-1-install-uv4l",
    "title": "Live Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone",
    "section": "Step 1: Install UV4L",
    "text": "Step 1: Install UV4L\n\nFollow the instructions here. For completeness, I will outline the steps I followed here.\nSince I have Raspbian Buster on my Pi, I used the following command\n\ncurl https://www.linux-projects.org/listing/uv4l_repo/lpkey.asc | sudo apt-key add -\n\nI add this line to the file /etc/apt/sources.list\n\ndeb https://www.linux-projects.org/listing/uv4l_repo/raspbian/stretch stretch main\n\nNext run the install commands\n\n$ sudo apt-get update\n$ sudo apt-get install uv4l uv4l-raspicam\nI stopped here as I just wanted to setup a video + audio stream for a monitoring application and did not really need the uv4l-raspicam-ai modules."
  },
  {
    "objectID": "posts/usb-camera-and-audio-streaming-pi/index.html#step-2-configure-usb-camera",
    "href": "posts/usb-camera-and-audio-streaming-pi/index.html#step-2-configure-usb-camera",
    "title": "Live Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone",
    "section": "Step 2: Configure USB camera",
    "text": "Step 2: Configure USB camera\n\nFollow the instructions on configuring UV4L or WebRTC using a USB camera here. Here are the steps I followed.\nInstalled the following packages\n\nsudo apt-get install uv4l uv4l-server uv4l-uvc uv4l-server uv4l-webrtc uv4l-xmpp-bridge\n\nUsed the dmesg command to find the USB camera device id\n\n$ dmesg\nAn excerpt from the output here:\n[    7.824796] mc: Linux media interface: v0.10\n[    7.889277] videodev: Linux video capture interface: v2.00\n[    8.072950] uvcvideo: Found UVC 1.00 device H264 USB Camera (05a3:9422)\n[    8.168458] input: H264 USB Camera: USB Camera as /devices/platform/soc/3f980000.usb/usb1/1-1/1-1.5/1-1.5:1.0/input/input8\n[    8.169582] usbcore: registered new interface driver uvcvideo\n[    8.169736] USB Video Class driver (1.1.1)\n\nRan this command to add the hex device ID on line three in the above output.\n\n$ uv4l --syslog-host localhost --driver uvc --device-id 05a3:9422\n\nI got this output indicating that the camera was indeed detected successfully.\n\n&lt;notice&gt; [core] Trying to loading driver 'uvc' from built-in drivers...\n&lt;notice&gt; [core] Loading driver 'uvc' from external plug-in's...\n&lt;notice&gt; [driver] Video functionality 'USB Camera' recognized at 05a3:9422\n&lt;notice&gt; [core] Device detected!\n&lt;notice&gt; [core] Registering device node /dev/uv4l\n\nChanged driver to uvc in /etc/uv4l/uv4l-raspicam.conf\n\ndriver = uvc\n\nReboot the Raspberry Pi using\n\n$ sudo reboot\nOnce the Pi is up, check the following url (note the port is not 8080! It‚Äôs http://pi_ip_address:8090/) on the client where you want to receive the stream ‚Äì in my case, this is my laptop. I used chrome as my browser. You should see a page like this.\n\n\n\nMain page of the UV4L streaming server\n\n\nClick on the WebRTC (first icon) on the page and you end up with a page where you can connect to the stream on the client.\n\n\n\nWebRTC page of the UV4L streaming server\n\n\nYou will see a button called Call (in green) in the bottom left part of the page. Just click it and you should start receiving the video stream like this. \n\nI have pointed the camera to the Pi since I wanted to check the delay in frames over my WiFi using the periodic blinking light on the LAN cable. The delay I noticed in video is acceptable for my purpose. As of now, you would notice that the audio is still not streaming."
  },
  {
    "objectID": "posts/usb-camera-and-audio-streaming-pi/index.html#step-3-configure-usb-microphone",
    "href": "posts/usb-camera-and-audio-streaming-pi/index.html#step-3-configure-usb-microphone",
    "title": "Live Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone",
    "section": "Step 3: Configure USB microphone",
    "text": "Step 3: Configure USB microphone\n\nFollow these instructions partially to find your USB microphone and include it in the WebRTC server configuration. Here are steps I followed.\nI executed the following command\n\n$ arecord -L | grep CARD\ndefault:CARD=Camera\nsysdefault:CARD=Camera\nfront:CARD=Camera,DEV=0\nsurround21:CARD=Camera,DEV=0\nsurround40:CARD=Camera,DEV=0\nsurround41:CARD=Camera,DEV=0\nsurround50:CARD=Camera,DEV=0\nsurround51:CARD=Camera,DEV=0\nsurround71:CARD=Camera,DEV=0\niec958:CARD=Camera,DEV=0\ndmix:CARD=Camera,DEV=0\ndsnoop:CARD=Camera,DEV=0\nhw:CARD=Camera,DEV=0\nplughw:CARD=Camera,DEV=0\ndefault:CARD=Microphones\nsysdefault:CARD=Microphones\nfront:CARD=Microphones,DEV=0\nsurround21:CARD=Microphones,DEV=0\nsurround40:CARD=Microphones,DEV=0\nsurround41:CARD=Microphones,DEV=0\nsurround50:CARD=Microphones,DEV=0\nsurround51:CARD=Microphones,DEV=0\nsurround71:CARD=Microphones,DEV=0\niec958:CARD=Microphones,DEV=0\ndmix:CARD=Microphones,DEV=0\ndsnoop:CARD=Microphones,DEV=0\nhw:CARD=Microphones,DEV=0\nplughw:CARD=Microphones,DEV=0\n\nA bit tricky, you have to literally count the lines starting from zero to your microphone device in the last line plughw:CARD=Microphones,DEV=0.\nplughw:CARD=Microphones,DEV=0 is line 27 starting from zero for the first line.\nIn the file /etc/uv4l/uv4l-raspicam.conf, find the line that has the audio device index and set the index to the line number of you microphone. In my case, it‚Äôs 27.\n\nserver-option = --webrtc-recdevice-index=27\nReboot the Raspberry Pi and you should be able to access the Video + Audio stream from Raspberry Pi on your WiFi at http:pi_ip_address:8090/stream/webrtc"
  },
  {
    "objectID": "posts/usb-camera-and-audio-streaming-pi/index.html#conclusion",
    "href": "posts/usb-camera-and-audio-streaming-pi/index.html#conclusion",
    "title": "Live Streaming Video + Audio on Raspberry Pi using USB Camera and Microphone",
    "section": "Conclusion",
    "text": "Conclusion\nThe setup of USB camera + USB microphone is something I had and wanted to utilize it without making additional purchases such as Pi Camera. This was a special requirement for me ‚Äì there may be much simpler tools if you have a Pi Camera with you. This setup looks very reliable with millisecond level latency. Also, the service has been running for over a day as of my writing of this article."
  },
  {
    "objectID": "posts/crazyflie-micro-uav/index.html",
    "href": "posts/crazyflie-micro-uav/index.html",
    "title": "Crazyflie2.1 Micro-UAV Assembly Experience",
    "section": "",
    "text": "I‚Äôm in the process of wrapping up my Flying Car and Autonomous Flight Engineer Udacity nanodegree and thought of sharing my experience in porting some of the projects to a real drone a.k.a. UAV (Unmanned Aerial Vehicle). However, before porting any code to a UAV, I had to buy an UAV. Deciding on a specific UAV to buy was a hard decision to make since there are so many options. Someone like me starting out in this space of UAVs would have many questions. Among them, two major questions I had were: Should I build my own UAV? OR Should I buy a ready-to-fly UAV?\nAfter some searching around, I realized that one should answer the question Do you plan to use the UAV indoors or outdoors? to make any buying decision. If your answer is indoors, a micro-UAV is a good option for safety and ease of maneuverability. Since I had no intention of flying the UAV outdoors, I preferred a micro-UAV. If your answer is outdoors, you can still get a micro-UAV but prefer a sturdy one so that it can withstand the cross-winds without crashing! There are great blogs such as How to Build FPV Drone outlining various choices and the building process.\nI chose Crazyflie since:"
  },
  {
    "objectID": "posts/crazyflie-micro-uav/index.html#getting-crazyflie",
    "href": "posts/crazyflie-micro-uav/index.html#getting-crazyflie",
    "title": "Crazyflie2.1 Micro-UAV Assembly Experience",
    "section": "Getting Crazyflie",
    "text": "Getting Crazyflie\nOnce I decided on the vehicle, I ordered Crazyflie2.1 from their website. I was excited and tracking it all the way from its home in Sweden to its new home here in Pittsburgh, United States. Crazyflie2.1 finally arrived!\n\n\n\nCrazyflie2.1 received from Crazyflie manufacturing location in Sweden"
  },
  {
    "objectID": "posts/crazyflie-micro-uav/index.html#assembly",
    "href": "posts/crazyflie-micro-uav/index.html#assembly",
    "title": "Crazyflie2.1 Micro-UAV Assembly Experience",
    "section": "Assembly",
    "text": "Assembly\nTo understand the rationale for the suggested assembly, please refer the Hot-swap battery arrangement section. We need an assembly that minimizes hardware stress when we want to replace the battery. This implies that we don‚Äôt want to pull out the multi-ranger deck every time we want to replace the battery.\nThere are already great instructions on assembly and making first flight on Crazyflie website here. Here, I will highlight the differences for the hot-swap friendly assembly.\nHere is the crazyflie base after partial assembly. You need to pay attention to the propeller naming and the direction of propeller rotation while performing this assembly (already described well here).\n\n\n\nCrazyflie2.1 after adding the arms, motors, and propellers\n\n\n\nAdding the flow-deck\nFlow deck allows the crazyflie to approximate it‚Äôs position along the horizontal direction (x and y direction) using a camera that faces down to the floor. This gives a ‚Äúsense‚Äù of lateral position awareness to Crazyflie. Further, the flow-deck has an additional sensor to measure height giving Crazyflie a ‚Äúsense‚Äù of height from the ground (z direction). With flow-deck added, Crazyflie now can approximate it‚Äôs 3D position when flying allowing it to be commanded to new position in 3D coordinates.\n\n\n\nCrazyflie2.1 flow-deck added facing down to the floor\n\n\n\n\nAdding the multi-ranger deck\nMulti-ranger deck is fitted on top of the Crazyflie but all the way down the pin unlike what is shown on Crazyflie website. That is, we place the battery on top of multi-ranger deck instead of under multi-ranger deck. One caveat is that Crazyflie cannot sense range in the upward direction ‚Äì this should be fine till you stay away from the ceiling of your indoor space.\n\n\n\nCrazyflie2.1 multi-ranger deck added on top and pushed all the way down\n\n\nA rubber strap securely holds the battery as shown enabling us to hot-swap the battery without messing with the multi-ranger deck. I had to learn this the hard way ‚Äì initially I had to remove the multi-ranger deck every time I had to replace the battery which was very time-consuming. This arrangement really helped me for making continuous flights by hot-swapping the battery with ease.\n\n\n\nCrazyflie2.1 with flow-deck, multi-ranger deck, and battery setup suitable for hot-swap ‚Äì it‚Äôs ready to fly"
  },
  {
    "objectID": "posts/crazyflie-micro-uav/index.html#hot-swap-battery-arrangement",
    "href": "posts/crazyflie-micro-uav/index.html#hot-swap-battery-arrangement",
    "title": "Crazyflie2.1 Micro-UAV Assembly Experience",
    "section": "Hot-swap battery arrangement",
    "text": "Hot-swap battery arrangement\nBattery supplied with Crazyflie2.1 is a 250 mAh battery with a flying time of around 7 minutes. This is good enough for indoor flights but if you like to skip waiting 40 minutes to charge the battery, one option is to hot-swap the battery (in other words, just replace the battery). I wanted to assemble the Crazyflie such that I can do this hot-swap with minimum effort. Specifically, I did not want to mess with the ranger deck every time I wanted to change the battery since in the standard configuration, the battery is firmly held in place by the ranger deck."
  },
  {
    "objectID": "posts/crazyflie-micro-uav/index.html#conclusion",
    "href": "posts/crazyflie-micro-uav/index.html#conclusion",
    "title": "Crazyflie2.1 Micro-UAV Assembly Experience",
    "section": "Conclusion",
    "text": "Conclusion\nI have been using the above assembly of Crazyflie2.1 for over four months now. I‚Äôm quite impressed by the hardware and firmware of Crazyflie2.1 ‚Äì especially, their python library is very well documented with examples. I have been able to port some of the projects from my nanodegree to work on Crazyflie. In the future posts, I will provide more details on porting motion planning and drone controls project to Crazyflie."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôm a Researcher and Engineer working on Machine Learning and Computer Vision related topics. I enjoy working on fun projects in my spare time. I like to bike and hike if I‚Äôm not doing any projects.\nI have experience developing research projects ground-up in both academia and industry. Here is my older webpage summarizing my PhD life.\nRecently, I have started to explore ideas of interactive teaching experiences to teach skills using deliberate practice principles. I strongly believe that meeting a good teacher will be a life changing experience for students. Keeping students motivated to learn is crucial and technology has a great potential to deliver great teaching and learning experiences."
  }
]